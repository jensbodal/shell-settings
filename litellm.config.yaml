# can i pass envvars to xtitle or whatever the http params is how
# uhh there was another comment
litellm_settings:
  default_team_settings:
    - team_id: "3e18e7c0-7988-4b5d-8af6-92dbe31c2c41"
      success_callback: ["prisma"]
      failure_callback: ["prisma"] # add ?
      #langfuse_public_key: os.environ/LANGFUSE_PUB_KEY_1 # Project 1
      #langfuse_secret: os.environ/LANGFUSE_PRIVATE_KEY_1 # Project 1

  default_team: "3e18e7c0-7988-4b5d-8af6-92dbe31c2c41"
  #success_callback: ["prisma"] # add langfuse and?
  #failure_callback: ["prisma"] # add ?
  json_logs: true
  #set_verbose: true # deprecated - use os.environ["LITELLM_LOG"] = "DEBUG" instead

# OR_SITE_URL handled via docker
# OR_APP_NAME handled via docker
#router_settings:

general_settings:
  litellm_key_header_name: 'Authorization'
  # @AI can we use gopass _here_ ?
  master_key: os.environ/LITELLM_MASTER_KEY
  database_url: os.environ/DATABASE_URL
  store_model_in_db: true
  #alerting: ["slack"] # [OPTIONAL] If you want Slack Alerts for Hanging LLM requests, Slow llm responses, Budget Alerts. Make sure to set `SLACK_WEBHOOK_URL` in your env

  # also curious about custom prompt template
  # https://docs.litellm.ai/docs/completion/input#input-params-1:~:text=the%20completion%20call-,CUSTOM,-PROMPT%20TEMPLATE%20(See

model_list:
  - model_name: auto
    litellm_params:
      model: openrouter/openrouter/auto
      api_key: os.environ/OPENROUTER_API_KEY
      api_base: https://openrouter.ai/api/v1

  - model_name: o3
    litellm_params:
      model: openrouter/openai/o3
      api_key: os.environ/OPENROUTER_API_KEY
      api_base: https://openrouter.ai/api/v1

  - model_name: mistral24bfree
    litellm_params:
      model: mistralai/mistral-small-3.2-24b-instruct:free
      api_key: os.environ/OPENROUTER_API_KEY
      api_base: https://openrouter.ai/api/v1

  - model_name: llama3-8b
    litellm_params:
      model: openrouter/meta-llama/llama-3-8b-instruct
      api_key: os.environ/OPENROUTER_API_KEY

  - model_name: bodal/qwen3         # <provider>/<model-tag>
    litellm_params:
      api_base:  http://localhost:11434   # Ollama REST endpoint
      model:     qwen3:latest                  # the tag you’ve pulled (e.g. llama2, mistral, phi3, …)

  - model_name: openrouter/*
    litellm_params:
      model: openrouter/*
      api_key: os.environ/OPENROUTER_API_KEY
      api_base: https://openrouter.ai/api/v1

  - model_name: "*"
    litellm_params:
      model: openrouter/openrouter/auto
      api_key: os.environ/OPENROUTER_API_KEY
      api_base: https://openrouter.ai/api/v1

virtual_key_list:
  - api_key: sk-my-virtual-key
    user_id: "jensbodal"
    team_id: "3e18e7c0-7988-4b5d-8af6-92dbe31c2c41"
