{
  "content": [
    {
      "type": "text",
      "text": "TITLE: Whisper CLI Usage and Options\nDESCRIPTION: Comprehensive list of command-line arguments and options for the whisper-cli program, including audio processing parameters, output formats, and model configuration settings. The CLI supports features like multi-threading, language detection, translation, diarization, and various output formats including TXT, VTT, SRT, LRC, CSV, and JSON.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/cli/README.md#2025-04-11_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n./build/bin/whisper-cli -h\n\nusage: ./build-pkg/bin/whisper-cli [options] file0.wav file1.wav ...\n\noptions:\n  -h,        --help              [default] show this help message and exit\n  -t N,      --threads N         [4      ] number of threads to use during computation\n  -p N,      --processors N      [1      ] number of processors to use during computation\n  -ot N,     --offset-t N        [0      ] time offset in milliseconds\n  -on N,     --offset-n N        [0      ] segment index offset\n  -d  N,     --duration N        [0      ] duration of audio to process in milliseconds\n  -mc N,     --max-context N     [-1     ] maximum number of text context tokens to store\n  -ml N,     --max-len N         [0      ] maximum segment length in characters\n  -sow,      --split-on-word     [false  ] split on word rather than on token\n  -bo N,     --best-of N         [5      ] number of best candidates to keep\n  -bs N,     --beam-size N       [5      ] beam size for beam search\n  -ac N,     --audio-ctx N       [0      ] audio context size (0 - all)\n  -wt N,     --word-thold N      [0.01   ] word timestamp probability threshold\n  -et N,     --entropy-thold N   [2.40   ] entropy threshold for decoder fail\n  -lpt N,    --logprob-thold N   [-1.00  ] log probability threshold for decoder fail\n  -tp,       --temperature N     [0.00   ] The sampling temperature, between 0 and 1\n  -tpi,      --temperature-inc N [0.20   ] The increment of temperature, between 0 and 1\n  -debug,    --debug-mode        [false  ] enable debug mode (eg. dump log_mel)\n  -tr,       --translate         [false  ] translate from source language to english\n  -di,       --diarize           [false  ] stereo audio diarization\n  -tdrz,     --tinydiarize       [false  ] enable tinydiarize (requires a tdrz model)\n  -nf,       --no-fallback       [false  ] do not use temperature fallback while decoding\n  -otxt,     --output-txt        [false  ] output result in a text file\n  -ovtt,     --output-vtt        [false  ] output result in a vtt file\n  -osrt,     --output-srt        [false  ] output result in a srt file\n  -olrc,     --output-lrc        [false  ] output result in a lrc file\n  -owts,     --output-words      [false  ] output script for generating karaoke video\n  -fp,       --font-path         [/System/Library/Fonts/Supplemental/Courier New Bold.ttf] path to a monospace font for karaoke video\n  -ocsv,     --output-csv        [false  ] output result in a CSV file\n  -oj,       --output-json       [false  ] output result in a JSON file\n  -ojf,      --output-json-full  [false  ] include more information in the JSON file\n  -of FNAME, --output-file FNAME [       ] output file path (without file extension)\n  -np,       --no-prints         [false  ] do not print anything other than the results\n  -ps,       --print-special     [false  ] print special tokens\n  -pc,       --print-colors      [false  ] print colors\n  -pp,       --print-progress    [false  ] print progress\n  -nt,       --no-timestamps     [false  ] do not print timestamps\n  -l LANG,   --language LANG     [en     ] spoken language ('auto' for auto-detect)\n  -dl,       --detect-language   [false  ] exit after automatically detecting language\n             --prompt PROMPT     [       ] initial prompt (max n_text_ctx/2 tokens)\n  -m FNAME,  --model FNAME       [models/ggml-base.en.bin] model path\n  -f FNAME,  --file FNAME        [       ] input WAV file path\n  -oved D,   --ov-e-device DNAME [CPU    ] the OpenVINO device used for encode inference\n  -dtw MODEL --dtw MODEL         [       ] compute token-level timestamps\n  -ls,       --log-score         [false  ] log best decoder scores of tokens\n  -ng,       --no-gpu            [false  ] disable GPU\n  -fa,       --flash-attn        [false  ] flash attention\n  --suppress-regex REGEX         [       ] regular expression matching tokens to suppress\n  --grammar GRAMMAR              [       ] GBNF grammar to guide decoding\n  --grammar-rule RULE            [       ] top-level GBNF grammar rule name\n  --grammar-penalty N            [100.0  ] scales down logits of nongrammar tokens\n```\n\n----------------------------------------\n\nTITLE: Building whisper.cpp with BLAS CPU Support\nDESCRIPTION: Configures and builds the project with CMake, enabling OpenBLAS support for CPU acceleration.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/README.md#2025-04-11_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\ncmake -B build -DGGML_BLAS=1\ncmake --build build -j --config Release\n```\n\n----------------------------------------\n\nTITLE: Cloning the Whisper.cpp Repository\nDESCRIPTION: This command clones the Whisper.cpp repository from GitHub to the local machine.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/README.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/ggml-org/whisper.cpp.git\n```\n\n----------------------------------------\n\nTITLE: Running whisper-stream with Sliding Window and VAD\nDESCRIPTION: Command to run whisper-stream in sliding window mode with Voice Activity Detection. Setting step to 0 enables the sliding window, while the -vth parameter controls the VAD threshold for speech detection.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/stream/README.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n./build/bin/whisper-stream -m ./models/ggml-base.en.bin -t 6 --step 0 --length 30000 -vth 0.6\n```\n\n----------------------------------------\n\nTITLE: Generating Word-level Timestamps with whisper-cli\nDESCRIPTION: Command-line example showing how to use the -ml 1 flag to generate word-level timestamps with whisper.cpp. The output shows a detailed breakdown of timestamps for each word in the transcription.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/README.md#2025-04-11_snippet_25\n\nLANGUAGE: text\nCODE:\n```\n$ ./build/bin/whisper-cli -m ./models/ggml-base.en.bin -f ./samples/jfk.wav -ml 1\n\nwhisper_model_load: loading model from './models/ggml-base.en.bin'\n...\nsystem_info: n_threads = 4 / 10 | AVX2 = 0 | AVX512 = 0 | NEON = 1 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 |\n\nmain: processing './samples/jfk.wav' (176000 samples, 11.0 sec), 4 threads, 1 processors, lang = en, task = transcribe, timestamps = 1 ...\n\n[00:00:00.000 --> 00:00:00.320]\n[00:00:00.320 --> 00:00:00.370]   And\n[00:00:00.370 --> 00:00:00.690]   so\n[00:00:00.690 --> 00:00:00.850]   my\n[00:00:00.850 --> 00:00:01.590]   fellow\n[00:00:01.590 --> 00:00:02.850]   Americans\n[00:00:02.850 --> 00:00:03.300]  ,\n[00:00:03.300 --> 00:00:04.140]   ask\n[00:00:04.140 --> 00:00:04.990]   not\n[00:00:04.990 --> 00:00:05.410]   what\n[00:00:05.410 --> 00:00:05.660]   your\n[00:00:05.660 --> 00:00:06.260]   country\n[00:00:06.260 --> 00:00:06.600]   can\n[00:00:06.600 --> 00:00:06.840]   do\n[00:00:06.840 --> 00:00:07.010]   for\n[00:00:07.010 --> 00:00:08.170]   you\n[00:00:08.170 --> 00:00:08.190]  ,\n[00:00:08.190 --> 00:00:08.430]   ask\n[00:00:08.430 --> 00:00:08.910]   what\n[00:00:08.910 --> 00:00:09.040]   you\n[00:00:09.040 --> 00:00:09.320]   can\n[00:00:09.320 --> 00:00:09.440]   do\n[00:00:09.440 --> 00:00:09.760]   for\n[00:00:09.760 --> 00:00:10.020]   your\n[00:00:10.020 --> 00:00:10.510]   country\n[00:00:10.510 --> 00:00:11.000]  .\n```\n\n----------------------------------------\n\nTITLE: Basic Transcription with Whisper in Ruby\nDESCRIPTION: Demonstrates how to initialize a Whisper context with a model, configure transcription parameters, and transcribe an audio file. Parameters include language specification, time offsets, token limits, translation options, and formatting preferences.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/bindings/ruby/README.md#2025-04-11_snippet_0\n\nLANGUAGE: ruby\nCODE:\n```\nrequire \"whisper\"\n\nwhisper = Whisper::Context.new(\"base\")\n\nparams = Whisper::Params.new(\n  language: \"en\",\n  offset: 10_000,\n  duration: 60_000,\n  max_text_tokens: 300,\n  translate: true,\n  print_timestamps: false,\n  initial_prompt: \"Initial prompt here.\"\n)\n\nwhisper.transcribe(\"path/to/audio.wav\", params) do |whole_text|\n  puts whole_text\nend\n```\n\n----------------------------------------\n\nTITLE: Building whisper-stream with SDL2 Support\nDESCRIPTION: Instructions for building the whisper-stream tool with SDL2 support for microphone capture. Includes package installation commands for different platforms and the necessary CMake commands.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/stream/README.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Install SDL2\n# On Debian based linux distributions:\nsudo apt-get install libsdl2-dev\n\n# On Fedora Linux:\nsudo dnf install SDL2 SDL2-devel\n\n# Install SDL2 on Mac OS\nbrew install sdl2\n\ncmake -B build -DWHISPER_SDL2=ON\ncmake --build build --config Release\n\n./build/bin/whisper-stream\n```\n\n----------------------------------------\n\nTITLE: Generating Karaoke-style Videos with whisper.cpp\nDESCRIPTION: Commands to generate karaoke-style videos where the currently pronounced word is highlighted. The example shows how to use the -owts flag to create a bash script that uses ffmpeg to produce the video.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/README.md#2025-04-11_snippet_27\n\nLANGUAGE: bash\nCODE:\n```\n./build/bin/whisper-cli -m ./models/ggml-base.en.bin -f ./samples/jfk.wav -owts\nsource ./samples/jfk.wav.wts\nffplay ./samples/jfk.wav.mp4\n```\n\nLANGUAGE: bash\nCODE:\n```\n./build/bin/whisper-cli -m ./models/ggml-base.en.bin -f ./samples/mm0.wav -owts\nsource ./samples/mm0.wav.wts\nffplay ./samples/mm0.wav.mp4\n```\n\nLANGUAGE: bash\nCODE:\n```\n./build/bin/whisper-cli -m ./models/ggml-base.en.bin -f ./samples/gb0.wav -owts\nsource ./samples/gb0.wav.wts\nffplay ./samples/gb0.wav.mp4\n```\n\n----------------------------------------\n\nTITLE: Transcribing Audio with Whisper-CLI\nDESCRIPTION: This command uses the built whisper-cli tool to transcribe an audio file (jfk.wav) using the Whisper model.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/README.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n./build/bin/whisper-cli -f samples/jfk.wav\n```\n\n----------------------------------------\n\nTITLE: Downloading Whisper Model in GGML Format\nDESCRIPTION: This script downloads a Whisper model converted to GGML format. The example uses the 'base.en' model.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/README.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nsh ./models/download-ggml-model.sh base.en\n```\n\n----------------------------------------\n\nTITLE: Setting up Python Environment for OpenVINO (Linux/macOS)\nDESCRIPTION: Creates a Python virtual environment and installs required dependencies for OpenVINO conversion on Linux and macOS.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/README.md#2025-04-11_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\ncd models\npython3 -m venv openvino_conv_env\nsource openvino_conv_env/bin/activate\npython -m pip install --upgrade pip\npip install -r requirements-openvino.txt\n```\n\n----------------------------------------\n\nTITLE: Initializing and Using WhisperCpp in Java\nDESCRIPTION: This snippet demonstrates how to initialize WhisperCpp, load a model, transcribe audio, and retrieve the transcribed text segments. It includes error handling and resource cleanup.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/bindings/java/README.md#2025-04-11_snippet_0\n\nLANGUAGE: java\nCODE:\n```\nimport io.github.ggerganov.whispercpp.WhisperCpp;\n\npublic class Example {\n\n    public static void main(String[] args) {\n        WhisperCpp whisper = new WhisperCpp();\n        // By default, models are loaded from ~/.cache/whisper/ and are usually named \"ggml-${name}.bin\"\n        // or you can provide the absolute path to the model file.\n        long context = whisper.initContext(\"base.en\");\n        try {\n            var whisperParams = whisper.getFullDefaultParams(WhisperSamplingStrategy.WHISPER_SAMPLING_GREEDY);\n            // custom configuration if required\n            whisperParams.temperature_inc = 0f;\n\n            var samples = readAudio(); // divide each value by 32767.0f\n            whisper.fullTranscribe(whisperParams, samples);\n\n            int segmentCount = whisper.getTextSegmentCount(context);\n            for (int i = 0; i < segmentCount; i++) {\n                String text = whisper.getTextSegment(context, i);\n                System.out.println(segment.getText());\n            }\n        } finally {\n             whisper.freeContext(context);\n        }\n     }\n}\n```\n\n----------------------------------------\n\nTITLE: Using Docker for Model Download and Transcription\nDESCRIPTION: Demonstrates how to use Docker to download a model and transcribe an audio file using whisper.cpp.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/README.md#2025-04-11_snippet_21\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -it --rm \\\n  -v path/to/models:/models \\\n  whisper.cpp:main \"./models/download-ggml-model.sh base /models\"\n\ndocker run -it --rm \\\n  -v path/to/models:/models \\\n  -v path/to/audios:/audios \\\n  whisper.cpp:main \"./main -m /models/ggml-base.bin -f /audios/jfk.wav\"\n```\n\n----------------------------------------\n\nTITLE: Using Whisper XCFramework in Swift Projects\nDESCRIPTION: Example of how to integrate the pre-built Whisper XCFramework into Swift projects using Swift Package Manager. This allows using the whisper.cpp library in iOS, visionOS, tvOS, and macOS applications without compiling from source.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/README.md#2025-04-11_snippet_30\n\nLANGUAGE: swift\nCODE:\n```\n// swift-tools-version: 5.10\n// The swift-tools-version declares the minimum version of Swift required to build this package.\n\nimport PackageDescription\n\nlet package = Package(\n    name: \"Whisper\",\n    targets: [\n        .executableTarget(\n            name: \"Whisper\",\n            dependencies: [\n                \"WhisperFramework\"\n            ]),\n        .binaryTarget(\n            name: \"WhisperFramework\",\n            url: \"https://github.com/ggml-org/whisper.cpp/releases/download/v1.7.5/whisper-v1.7.5-xcframework.zip\",\n            checksum: \"c7faeb328620d6012e130f3d705c51a6ea6c995605f2df50f6e1ad68c59c6c4a\"\n        )\n    ]\n)\n```\n\n----------------------------------------\n\nTITLE: Building Whisper.cpp Project with CMake\nDESCRIPTION: These commands build the Whisper.cpp project using CMake, creating a build directory and compiling the project in Release configuration.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/README.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncmake -B build\ncmake --build build --config Release\n```\n\n----------------------------------------\n\nTITLE: Converting Audio to 16-bit WAV Format\nDESCRIPTION: This ffmpeg command converts an input audio file to a 16-bit WAV format compatible with the whisper-cli tool.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/README.md#2025-04-11_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nffmpeg -i input.mp3 -ar 16000 -ac 1 -c:a pcm_s16le output.wav\n```\n\n----------------------------------------\n\nTITLE: Using Pre-converted Whisper Models in Ruby\nDESCRIPTION: Shows how to use pre-converted Whisper models, which are downloaded automatically on first use and then cached. Includes methods for accessing cached models, clearing the cache, and listing available pre-converted models.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/bindings/ruby/README.md#2025-04-11_snippet_1\n\nLANGUAGE: ruby\nCODE:\n```\nbase_en = Whisper::Model.pre_converted_models[\"base.en\"]\nwhisper = Whisper::Context.new(base_en)\n```\n\nLANGUAGE: ruby\nCODE:\n```\nWhisper::Model.pre_converted_models[\"base\"].clear_cache\n```\n\nLANGUAGE: ruby\nCODE:\n```\nwhisper = Whisper::Context.new(\"base.en\")\n```\n\nLANGUAGE: ruby\nCODE:\n```\nputs Whisper::Model.pre_converted_models.keys\n# tiny\n# tiny.en\n# tiny-q5_1\n# tiny.en-q5_1\n# tiny-q8_0\n# base\n# base.en\n# base-q5_1\n# base.en-q5_1\n# base-q8_0\n#   :\n#   :\n```\n\n----------------------------------------\n\nTITLE: Using Segment Callbacks in Whisper Ruby\nDESCRIPTION: Demonstrates how to register a callback that is triggered on each new segment during transcription. The callback receives segment information including text, timestamps, and speaker change indicators.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/bindings/ruby/README.md#2025-04-11_snippet_4\n\nLANGUAGE: ruby\nCODE:\n```\n# Add hook before calling #transcribe\nparams.on_new_segment do |segment|\n  line = \"[%{st} --> %{ed}] %{text}\" % {\n    st: format_time(segment.start_time),\n    ed: format_time(segment.end_time),\n    text: segment.text\n  }\n  line << \" (speaker turned)\" if segment.speaker_next_turn?\n  puts line\nend\n\nwhisper.transcribe(\"path/to/audio.wav\", params)\n```\n\n----------------------------------------\n\nTITLE: Running Basic Real-Time Transcription with whisper-stream\nDESCRIPTION: Command to run the whisper-stream tool for real-time audio transcription from the microphone with a fixed step interval. This samples audio every 500ms and processes 5-second chunks using 8 threads.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/stream/README.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./build/bin/whisper-stream -m ./models/ggml-base.en.bin -t 8 --step 500 --length 5000\n```\n\n----------------------------------------\n\nTITLE: Downloading the Whisper Base Model\nDESCRIPTION: Command to download the English base model for Whisper, which is required for transcription.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/whisper.nvim/README.md#2025-04-11_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n./models/download-ggml-model.sh base.en\n```\n\n----------------------------------------\n\nTITLE: Converting PyTorch Models to ggml Format\nDESCRIPTION: Bash script demonstrating how to convert a PyTorch Whisper model to ggml format using the convert-pt-to-ggml.py script. The process includes creating a directory, running the conversion script, and moving the output file.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/models/README.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmkdir models/whisper-medium\npython models/convert-pt-to-ggml.py ~/.cache/whisper/medium.pt ~/path/to/repo/whisper/ ./models/whisper-medium\nmv ./models/whisper-medium/ggml-model.bin models/ggml-medium.bin\nrmdir models/whisper-medium\n```\n\n----------------------------------------\n\nTITLE: Downloading ggml Models with download-ggml-model.sh\nDESCRIPTION: Example of downloading a pre-converted ggml model using the download-ggml-model.sh script. The command downloads the base.en model and outputs information about how to use it.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/models/README.md#2025-04-11_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n$ ./download-ggml-model.sh base.en\nDownloading ggml model base.en ...\nmodels/ggml-base.en.bin          100%[=============================================>] 141.11M  5.41MB/s    in 22s\nDone! Model 'base.en' saved in 'models/ggml-base.en.bin'\nYou can now use it like this:\n\n  $ ./build/bin/whisper-cli -m models/ggml-base.en.bin -f samples/jfk.wav\n```\n\n----------------------------------------\n\nTITLE: Defining Build Options for whisper.cpp\nDESCRIPTION: Defines the available build options for the project, including general settings, debug options, sanitizers, and third-party library integrations. These options control the build behavior and feature set of whisper.cpp.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/CMakeLists.txt#2025-04-11_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\n#\n# option list\n#\n\n# general\noption(WHISPER_CCACHE \"whisper: use ccache if available\" ON)\n\n# debug\noption(WHISPER_ALL_WARNINGS           \"whisper: enable all compiler warnings\"                   ON)\noption(WHISPER_ALL_WARNINGS_3RD_PARTY \"whisper: enable all compiler warnings in 3rd party libs\" OFF)\n\n# build\noption(WHISPER_FATAL_WARNINGS  \"whisper: enable -Werror flag\"               OFF)\noption(WHISPER_USE_SYSTEM_GGML \"whisper: use system-installed GGML library\" OFF)\n\n# sanitizers\noption(WHISPER_SANITIZE_THREAD    \"whisper: enable thread sanitizer\"    OFF)\noption(WHISPER_SANITIZE_ADDRESS   \"whisper: enable address sanitizer\"   OFF)\noption(WHISPER_SANITIZE_UNDEFINED \"whisper: enable undefined sanitizer\" OFF)\n\n# extra artifacts\noption(WHISPER_BUILD_TESTS    \"whisper: build tests\"          ${WHISPER_STANDALONE})\noption(WHISPER_BUILD_EXAMPLES \"whisper: build examples\"       ${WHISPER_STANDALONE})\noption(WHISPER_BUILD_SERVER   \"whisper: build server example\" ${WHISPER_STANDALONE})\n\n# 3rd party libs\noption(WHISPER_CURL \"whisper: use libcurl to download model from an URL\" OFF)\noption(WHISPER_SDL2 \"whisper: support for libSDL2\" OFF)\n\nif (CMAKE_SYSTEM_NAME MATCHES \"Linux\")\n    option(WHISPER_FFMPEG \"whisper: support building and linking with ffmpeg libs (avcodec, swresample, ...)\" OFF)\nendif()\n\noption(WHISPER_COREML                \"whisper: enable Core ML framework\"  OFF)\noption(WHISPER_COREML_ALLOW_FALLBACK \"whisper: allow non-CoreML fallback\" OFF)\noption(WHISPER_OPENVINO              \"whisper: support for OpenVINO\"      OFF)\n```\n\n----------------------------------------\n\nTITLE: Building and Running Voice-Controlled Chess in Bash\nDESCRIPTION: Commands for building the wchess project from source using CMake and running it with a Whisper model. The resulting program displays a chess board interface in the terminal and accepts voice commands for moves.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/wchess/README.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmkdir build && cd build\ncmake -DWHISPER_SDL2=1 ..\nmake -j\n\n./bin/wchess -m ../models/ggml-base.en.bin\n\nMove: start\n\na b c d e f g h\nr n b q k b n r 8\np p p p p p p p 7\n. * . * . * . * 6\n* . * . * . * . 5\n. * . * . * . * 4\n* . * . * . * . 3\nP P P P P P P P 2\nR N B Q K B N R 1\n\nWhite's turn\n[(l)isten/(p)ause/(q)uit]: \n```\n\n----------------------------------------\n\nTITLE: Loading Custom Whisper Models in Ruby\nDESCRIPTION: Demonstrates how to use local model files and remotely hosted models in the Whisper context. Supports loading from local paths or remote URLs.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/bindings/ruby/README.md#2025-04-11_snippet_2\n\nLANGUAGE: ruby\nCODE:\n```\nwhisper = Whisper::Context.new(\"path/to/your/model.bin\")\n```\n\nLANGUAGE: ruby\nCODE:\n```\nwhisper = Whisper::Context.new(\"https://example.net/uri/of/your/model.bin\")\n# Or\nwhisper = Whisper::Context.new(URI(\"https://example.net/uri/of/your/model.bin\"))\n```\n\n----------------------------------------\n\nTITLE: Building Whisper.wasm with Emscripten\nDESCRIPTION: This snippet shows how to clone the whisper.cpp repository, create a build directory, and compile the project using Emscripten for WebAssembly output. It uses CMake for build configuration.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/whisper.wasm/README.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# build using Emscripten\ngit clone https://github.com/ggml-org/whisper.cpp\ncd whisper.cpp\nmkdir build-em && cd build-em\nemcmake cmake ..\nmake -j\n```\n\n----------------------------------------\n\nTITLE: Running Whisper.cpp Node.js Addon Example\nDESCRIPTION: Command to run the example Node.js script that uses the compiled Whisper.cpp addon. It demonstrates how to pass language, model path, and input file path as command-line arguments.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/addon.node/README.md#2025-04-11_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ncd examples/addon.node\n\nnode index.js --language='language' --model='model-path' --fname_inp='file-path'\n```\n\n----------------------------------------\n\nTITLE: Accessing Whisper Model Information in Ruby\nDESCRIPTION: Shows how to retrieve detailed information about the loaded Whisper model, including vocabulary size, context dimensions, network architecture parameters, and model type.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/bindings/ruby/README.md#2025-04-11_snippet_5\n\nLANGUAGE: ruby\nCODE:\n```\nwhisper = Whisper::Context.new(\"base\")\nmodel = whisper.model\n\nmodel.n_vocab # => 51864\nmodel.n_audio_ctx # => 1500\nmodel.n_audio_state # => 512\nmodel.n_audio_head # => 8\nmodel.n_audio_layer # => 6\nmodel.n_text_ctx # => 448\nmodel.n_text_state # => 512\nmodel.n_text_head # => 8\nmodel.n_text_layer # => 6\nmodel.n_mels # => 80\nmodel.ftype # => 1\nmodel.type # => \"base\"\n```\n\n----------------------------------------\n\nTITLE: Configuring WebAssembly Build Target for Whisper.cpp using CMake\nDESCRIPTION: This CMake script configures the compilation of the Whisper speech recognition library to WebAssembly. It defines the target executable, links the Whisper library, and sets up Emscripten-specific link flags for thread support, memory management, and module export. The script also includes an optional configuration to embed the WASM binary directly in the JavaScript file for simpler deployment.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/bindings/javascript/CMakeLists.txt#2025-04-11_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET libwhisper)\n\nadd_executable(${TARGET}\n    emscripten.cpp\n    )\n\ntarget_link_libraries(${TARGET} PRIVATE\n    whisper\n    )\n\nunset(EXTRA_FLAGS)\n\nif (WHISPER_WASM_SINGLE_FILE)\n    set(EXTRA_FLAGS \"-s SINGLE_FILE=1\")\n    message(STATUS \"Embedding WASM inside whisper.js\")\n\n    add_custom_command(\n        TARGET ${TARGET} POST_BUILD\n        COMMAND ${CMAKE_COMMAND} -E copy\n        ${CMAKE_BINARY_DIR}/bin/libwhisper.js\n        ${CMAKE_CURRENT_SOURCE_DIR}/whisper.js\n        )\n\n    add_custom_command(\n        TARGET ${TARGET} POST_BUILD\n        COMMAND ${CMAKE_COMMAND} -E copy\n        ${CMAKE_BINARY_DIR}/bin/libwhisper.worker.js\n        ${CMAKE_CURRENT_SOURCE_DIR}/libwhisper.worker.js\n        )\nendif()\n\nset_target_properties(${TARGET} PROPERTIES LINK_FLAGS \" \\\n    --bind \\\n    -s MODULARIZE=1 \\\n    -s EXPORT_NAME=\\\"'whisper_factory'\\\" \\\n    -s FORCE_FILESYSTEM=1 \\\n    -s USE_PTHREADS=1 \\\n    -s PTHREAD_POOL_SIZE=8 \\\n    -s ALLOW_MEMORY_GROWTH=1 \\\n    ${EXTRA_FLAGS} \\\n    \")\n```\n\n----------------------------------------\n\nTITLE: Generating Core ML Model for Whisper\nDESCRIPTION: This script generates a Core ML model for the 'base.en' Whisper model, which can be used for accelerated inference on Apple Silicon devices.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/README.md#2025-04-11_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n./models/generate-coreml-model.sh base.en\n```\n\n----------------------------------------\n\nTITLE: Building and Running Real-time Audio Input Example\nDESCRIPTION: Configures, builds, and runs the real-time audio input example using SDL2.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/README.md#2025-04-11_snippet_22\n\nLANGUAGE: bash\nCODE:\n```\ncmake -B build -DWHISPER_SDL2=ON\ncmake --build build --config Release\n./build/bin/whisper-stream -m ./models/ggml-base.en.bin -t 8 --step 500 --length 5000\n```\n\n----------------------------------------\n\nTITLE: Speaker Segmentation using tinydiarize in whisper.cpp\nDESCRIPTION: Example of using tinydiarize for speaker segmentation in whisper.cpp. It demonstrates how to download a compatible model and run it with the -tdrz flag, which adds speaker turn annotations to the transcription.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/README.md#2025-04-11_snippet_26\n\nLANGUAGE: python\nCODE:\n```\n# download a tinydiarize compatible model\n./models/download-ggml-model.sh small.en-tdrz\n\n# run as usual, adding the \"-tdrz\" command-line argument\n./build/bin/whisper-cli -f ./samples/a13.wav -m ./models/ggml-small.en-tdrz.bin -tdrz\n...\nmain: processing './samples/a13.wav' (480000 samples, 30.0 sec), 4 threads, 1 processors, lang = en, task = transcribe, tdrz = 1, timestamps = 1 ...\n...\n[00:00:00.000 --> 00:00:03.800]   Okay Houston, we've had a problem here. [SPEAKER_TURN]\n[00:00:03.800 --> 00:00:06.200]   This is Houston. Say again please. [SPEAKER_TURN]\n[00:00:06.200 --> 00:00:08.260]   Uh Houston we've had a problem.\n[00:00:08.260 --> 00:00:11.320]   We've had a main beam up on a volt. [SPEAKER_TURN]\n[00:00:11.320 --> 00:00:13.820]   Roger main beam interval. [SPEAKER_TURN]\n[00:00:13.820 --> 00:00:15.100]   Uh uh [SPEAKER_TURN]\n[00:00:15.100 --> 00:00:18.020]   So okay stand, by thirteen we're looking at it. [SPEAKER_TURN]\n[00:00:18.020 --> 00:00:25.740]   Okay uh right now uh Houston the uh voltage is uh is looking good um.\n[00:00:27.620 --> 00:00:29.940]   And we had a a pretty large bank or so.\n```\n\n----------------------------------------\n\nTITLE: Compiling Whisper.cpp Node.js Addon with cmake-js\nDESCRIPTION: Command to compile the Whisper.cpp Node.js addon using cmake-js. It specifies the target name as 'addon.node' and sets the build type to 'Release'.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/addon.node/README.md#2025-04-11_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nnpx cmake-js compile -T addon.node -B Release\n```\n\n----------------------------------------\n\nTITLE: Running the Whisper Benchmarking Tool with a Small English Model\nDESCRIPTION: This command demonstrates how to run the whisper-bench tool on the small.en model using 4 threads. The output shows detailed model information and performance metrics, including load time, encoding time per layer, and total execution time. The benchmark results can be submitted to a GitHub issue for comparison.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/bench/README.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# run the bench too on the small.en model using 4 threads\n$ ./build/bin/whisper-bench -m ./models/ggml-small.en.bin -t 4\n\nwhisper_model_load: loading model from './models/ggml-small.en.bin'\nwhisper_model_load: n_vocab       = 51864\nwhisper_model_load: n_audio_ctx   = 1500\nwhisper_model_load: n_audio_state = 768\nwhisper_model_load: n_audio_head  = 12\nwhisper_model_load: n_audio_layer = 12\nwhisper_model_load: n_text_ctx    = 448\nwhisper_model_load: n_text_state  = 768\nwhisper_model_load: n_text_head   = 12\nwhisper_model_load: n_text_layer  = 12\nwhisper_model_load: n_mels        = 80\nwhisper_model_load: f16           = 1\nwhisper_model_load: type          = 3\nwhisper_model_load: mem_required  = 1048.00 MB\nwhisper_model_load: adding 1607 extra tokens\nwhisper_model_load: ggml ctx size = 533.05 MB\nwhisper_model_load: memory size =    68.48 MB \nwhisper_model_load: model size  =   464.44 MB\n\nwhisper_print_timings:     load time =   240.82 ms\nwhisper_print_timings:      mel time =     0.00 ms\nwhisper_print_timings:   sample time =     0.00 ms\nwhisper_print_timings:   encode time =  1062.21 ms / 88.52 ms per layer\nwhisper_print_timings:   decode time =     0.00 ms / 0.00 ms per layer\nwhisper_print_timings:    total time =  1303.04 ms\n\nsystem_info: n_threads = 4 | AVX2 = 0 | AVX512 = 0 | NEON = 1 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | \n\nIf you wish, you can submit these results here:\n\n  https://github.com/ggml-org/whisper.cpp/issues/89\n\nPlease include the following information:\n\n  - CPU model\n  - Operating system\n  - Compiler\n```\n\n----------------------------------------\n\nTITLE: Building whisper.cpp with NVIDIA GPU Support\nDESCRIPTION: Configures and builds the project with CMake, enabling CUDA support for NVIDIA GPUs.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/README.md#2025-04-11_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\ncmake -B build -DGGML_CUDA=1\ncmake --build build -j --config Release\n```\n\n----------------------------------------\n\nTITLE: Converting Hugging Face Fine-tuned Models to ggml Format\nDESCRIPTION: Bash script showing how to convert Hugging Face fine-tuned Whisper models to ggml format. The process includes cloning necessary repositories, downloading the model, and running the conversion script.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/models/README.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/openai/whisper\ngit clone https://github.com/ggml-org/whisper.cpp\n\n# clone HF fine-tuned model (this is just an example)\ngit clone https://huggingface.co/openai/whisper-medium\n\n# convert the model to ggml\npython3 ./whisper.cpp/models/convert-h5-to-ggml.py ./whisper-medium/ ./whisper .\n```\n\n----------------------------------------\n\nTITLE: Controlling Text Segment Length in Transcription\nDESCRIPTION: Demonstrates how to limit the length of generated text segments during transcription.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/README.md#2025-04-11_snippet_24\n\nLANGUAGE: bash\nCODE:\n```\n./build/bin/whisper-cli -m ./models/ggml-base.en.bin -f ./samples/jfk.wav -ml 16\n```\n\n----------------------------------------\n\nTITLE: Sample Output of Whisper.cpp Node.js Test Run\nDESCRIPTION: Example output showing the model loading process, system information, and transcription results from running the test script. Includes detailed timing metrics for different processing stages.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/bindings/javascript/README.md#2025-04-11_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n$ node --experimental-wasm-threads --experimental-wasm-simd ../tests/test-whisper.js\n\nwhisper_model_load: loading model from 'whisper.bin'\nwhisper_model_load: n_vocab       = 51864\nwhisper_model_load: n_audio_ctx   = 1500\nwhisper_model_load: n_audio_state = 512\nwhisper_model_load: n_audio_head  = 8\nwhisper_model_load: n_audio_layer = 6\nwhisper_model_load: n_text_ctx    = 448\nwhisper_model_load: n_text_state  = 512\nwhisper_model_load: n_text_head   = 8\nwhisper_model_load: n_text_layer  = 6\nwhisper_model_load: n_mels        = 80\nwhisper_model_load: f16           = 1\nwhisper_model_load: type          = 2\nwhisper_model_load: adding 1607 extra tokens\nwhisper_model_load: mem_required  =  506.00 MB\nwhisper_model_load: ggml ctx size =  140.60 MB\nwhisper_model_load: memory size   =   22.83 MB\nwhisper_model_load: model size    =  140.54 MB\n\nsystem_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | NEON = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 1 | BLAS = 0 |\n\noperator(): processing 176000 samples, 11.0 sec, 8 threads, 1 processors, lang = en, task = transcribe ...\n\n[00:00:00.000 --> 00:00:11.000]   And so my fellow Americans, ask not what your country can do for you, ask what you can do for your country.\n\nwhisper_print_timings:     load time =   162.37 ms\nwhisper_print_timings:      mel time =   183.70 ms\nwhisper_print_timings:   sample time =     4.27 ms\nwhisper_print_timings:   encode time =  8582.63 ms / 1430.44 ms per layer\nwhisper_print_timings:   decode time =   436.16 ms / 72.69 ms per layer\nwhisper_print_timings:    total time =  9370.90 ms\n```\n\n----------------------------------------\n\nTITLE: Building whisper.cpp with FFmpeg Support (Linux)\nDESCRIPTION: Configures and builds the project with CMake, enabling FFmpeg integration for additional audio format support on Linux.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/README.md#2025-04-11_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\ncmake -B build -D WHISPER_FFMPEG=yes\ncmake --build build\n```\n\n----------------------------------------\n\nTITLE: Building whisper.cpp with Vulkan GPU Support\nDESCRIPTION: Configures and builds the project with CMake, enabling Vulkan support for cross-vendor GPU acceleration.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/README.md#2025-04-11_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\ncmake -B build -DGGML_VULKAN=1\ncmake --build build -j --config Release\n```\n\n----------------------------------------\n\nTITLE: Configuring and Building Whisper Server with CMake\nDESCRIPTION: Sets up the whisper-server target by defining source files, including dependencies, linking libraries, and handling platform-specific requirements. The configuration links the common, json_cpp, and whisper libraries, adds threading support, and includes Windows-specific socket libraries when building on Windows.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/server/CMakeLists.txt#2025-04-11_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(TARGET whisper-server)\nadd_executable(${TARGET} server.cpp httplib.h)\n\ninclude(DefaultTargetOptions)\n\ntarget_link_libraries(${TARGET} PRIVATE common json_cpp whisper ${CMAKE_THREAD_LIBS_INIT})\n\nif (WIN32)\n    target_link_libraries(${TARGET} PRIVATE ws2_32)\nendif()\n\ninstall(TARGETS ${TARGET} RUNTIME)\n```\n\n----------------------------------------\n\nTITLE: Running Transcription with Confidence Color-coding\nDESCRIPTION: Executes the whisper-cli with color-coded confidence output for transcribed text.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/README.md#2025-04-11_snippet_23\n\nLANGUAGE: bash\nCODE:\n```\n./build/bin/whisper-cli -m models/ggml-base.en.bin -f samples/gb0.wav --print-colors\n```\n\n----------------------------------------\n\nTITLE: Building whisper.cpp with OpenVINO Support\nDESCRIPTION: Configures and builds the project with CMake, enabling OpenVINO support.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/README.md#2025-04-11_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\ncmake -B build -DWHISPER_OPENVINO=1\ncmake --build build -j --config Release\n```\n\n----------------------------------------\n\nTITLE: Whisper Server Model Loading API Request\nDESCRIPTION: cURL command example for loading a new model file into the server.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/server/README.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncurl 127.0.0.1:8080/load \\\n-H \"Content-Type: multipart/form-data\" \\\n-F model=\"<path-to-model-file>\"\n```\n\n----------------------------------------\n\nTITLE: Building Whisper Command with SDL2 Dependencies\nDESCRIPTION: Instructions for building the whisper-command tool with SDL2 library dependencies across different operating systems including Debian, Fedora, and MacOS.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/command/README.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Install SDL2\n# On Debian based linux distributions:\nsudo apt-get install libsdl2-dev\n\n# On Fedora Linux:\nsudo dnf install SDL2 SDL2-devel\n\n# Install SDL2 on Mac OS\nbrew install sdl2\n\ncmake -B build -DWHISPER_SDL2=ON\ncmake --build build --config Release\n```\n\n----------------------------------------\n\nTITLE: Building Whisper.cpp with Core ML Support\nDESCRIPTION: These commands build Whisper.cpp with Core ML support enabled, allowing for accelerated inference on Apple Silicon devices.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/README.md#2025-04-11_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\ncmake -B build -DWHISPER_COREML=1\ncmake --build build -j --config Release\n```\n\n----------------------------------------\n\nTITLE: Creating Video Comparisons of Different Whisper Models\nDESCRIPTION: Commands to generate a video that compares the transcription quality of different whisper.cpp models on the same audio input. Uses a dedicated script and ffmpeg to produce a side-by-side comparison.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/README.md#2025-04-11_snippet_28\n\nLANGUAGE: bash\nCODE:\n```\n./scripts/bench-wts.sh samples/jfk.wav\nffplay ./samples/jfk.wav.all.mp4\n```\n\n----------------------------------------\n\nTITLE: Building and Running whisper-talk-llama with SDL2\nDESCRIPTION: Instructions for installing SDL2 dependencies, building the whisper-talk-llama executable, and running it with specific command line arguments for Whisper and LLaMA models.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/talk-llama/README.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Install SDL2\n# On Debian based linux distributions:\nsudo apt-get install libsdl2-dev\n\n# On Fedora Linux:\nsudo dnf install SDL2 SDL2-devel\n\n# Install SDL2 on Mac OS\nbrew install sdl2\n\n# Build the \"whisper-talk-llama\" executable\ncmake -B build -S . -DWHISPER_SDL2=ON\ncmake --build build --config Release\n\n# Run it\n./build/bin/whisper-talk-llama -mw ./models/ggml-small.en.bin -ml ../llama.cpp/models/llama-13b/ggml-model-q4_0.gguf -p \"Georgi\" -t 8\n```\n\n----------------------------------------\n\nTITLE: Running Whisper Command with Default Settings\nDESCRIPTION: Commands for running the voice assistant with default arguments and small model. Includes specific optimization parameters for Raspberry Pi deployment.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/command/README.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Run with default arguments and small model\n./whisper-command -m ./models/ggml-small.en.bin -t 8\n\n# On Raspberry Pi, use tiny or base models + \"-ac 768\" for better performance\n./whisper-command -m ./models/ggml-tiny.en.bin -ac 768 -t 3 -c 0\n```\n\n----------------------------------------\n\nTITLE: Whisper Server Command Line Options\nDESCRIPTION: Comprehensive list of command-line arguments for configuring the whisper.cpp server, including threading, processing, model, and server options.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/server/README.md#2025-04-11_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n./build/bin/whisper-server -h\n\nusage: ./build/bin/whisper-server [options]\n\noptions:\n  -h,        --help              [default] show this help message and exit\n  -t N,      --threads N         [4      ] number of threads to use during computation\n  -p N,      --processors N      [1      ] number of processors to use during computation\n  -ot N,     --offset-t N        [0      ] time offset in milliseconds\n  -on N,     --offset-n N        [0      ] segment index offset\n  -d  N,     --duration N        [0      ] duration of audio to process in milliseconds\n  -mc N,     --max-context N     [-1     ] maximum number of text context tokens to store\n  -ml N,     --max-len N         [0      ] maximum segment length in characters\n  -sow,      --split-on-word     [false  ] split on word rather than on token\n  -bo N,     --best-of N         [2      ] number of best candidates to keep\n  -bs N,     --beam-size N       [-1     ] beam size for beam search\n  -wt N,     --word-thold N      [0.01   ] word timestamp probability threshold\n  -et N,     --entropy-thold N   [2.40   ] entropy threshold for decoder fail\n  -lpt N,    --logprob-thold N   [-1.00  ] log probability threshold for decoder fail\n  -debug,    --debug-mode        [false  ] enable debug mode (eg. dump log_mel)\n  -tr,       --translate         [false  ] translate from source language to english\n  -di,       --diarize           [false  ] stereo audio diarization\n  -tdrz,     --tinydiarize       [false  ] enable tinydiarize (requires a tdrz model)\n  -nf,       --no-fallback       [false  ] do not use temperature fallback while decoding\n  -ps,       --print-special     [false  ] print special tokens\n  -pc,       --print-colors      [false  ] print colors\n  -pr,       --print-realtime    [false  ] print output in realtime\n  -pp,       --print-progress    [false  ] print progress\n  -nt,       --no-timestamps     [false  ] do not print timestamps\n  -l LANG,   --language LANG     [en     ] spoken language ('auto' for auto-detect)\n  -dl,       --detect-language   [false  ] exit after automatically detecting language\n             --prompt PROMPT     [       ] initial prompt\n  -m FNAME,  --model FNAME       [models/ggml-base.en.bin] model path\n  -oved D,   --ov-e-device DNAME [CPU    ] the OpenVINO device used for encode inference\n  --host HOST,                   [127.0.0.1] Hostname/ip-adress for the server\n  --port PORT,                   [8080   ] Port number for the server\n  --convert,                     [false  ] Convert audio to WAV, requires ffmpeg on the server\n```\n\n----------------------------------------\n\nTITLE: Building and Testing Whisper.cpp Java Bindings\nDESCRIPTION: This bash script demonstrates how to clone the whisper.cpp repository, navigate to the Java bindings directory, and run the Gradle build process for testing the Java bindings.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/bindings/java/README.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/ggml-org/whisper.cpp.git\ncd whisper.cpp/bindings/java\n\n./gradlew build\n```\n\n----------------------------------------\n\nTITLE: Quantizing Whisper Model\nDESCRIPTION: This command quantizes a Whisper model using the Q5_0 method, which reduces model size and can improve inference speed.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/README.md#2025-04-11_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n./build/bin/quantize models/ggml-base.en.bin models/ggml-base.en-q5_0.bin q5_0\n```\n\n----------------------------------------\n\nTITLE: GGML Backend Configuration Options\nDESCRIPTION: Defines CMake options for various hardware acceleration backends and their settings, including CUDA, Metal, Vulkan, OpenCL, and others. Each option controls specific features and behaviors of the respective backend.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/CMakeLists.txt#2025-04-11_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\noption(GGML_ACCELERATE \"ggml: enable Accelerate framework\" ON)\noption(GGML_BLAS \"ggml: use BLAS\" ${GGML_BLAS_DEFAULT})\nset(GGML_BLAS_VENDOR ${GGML_BLAS_VENDOR_DEFAULT} CACHE STRING \"ggml: BLAS library vendor\")\noption(GGML_LLAMAFILE \"ggml: use LLAMAFILE\" ${GGML_LLAMAFILE_DEFAULT})\n```\n\n----------------------------------------\n\nTITLE: Downloading LibriSpeech Audio Files\nDESCRIPTION: Command to download the necessary audio files from the LibriSpeech project for testing purposes.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/tests/librispeech/README.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ make get-audio\n```\n\n----------------------------------------\n\nTITLE: Running whisper-talk-llama with Session Support\nDESCRIPTION: Example command for running whisper-talk-llama with session management enabled, which allows for maintaining conversation context across multiple interactions by saving and loading model state.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/talk-llama/README.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n./build/bin/whisper-talk-llama --session ./my-session-file -mw ./models/ggml-small.en.bin -ml ../llama.cpp/models/llama-13b/ggml-model-q4_0.gguf -p \"Georgi\" -t 8\n```\n\n----------------------------------------\n\nTITLE: Building whisper.cpp from the Project Root Directory\nDESCRIPTION: Commands to compile the whisper-cli executable and download a model in ggml format. These steps are prerequisites for running the LibriSpeech tests.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/tests/librispeech/README.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ # Execute the commands below in the project root dir.\n$ cmake -B build\n$ cmake --build build --config Release\n$ ./models/download-ggml-model.sh tiny\n```\n\n----------------------------------------\n\nTITLE: Starting a Local HTTP Server for Whisper.wasm\nDESCRIPTION: This snippet demonstrates how to start a local HTTP server using Python to serve the Whisper.wasm example. It also provides the URL to access the example in a web browser.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/whisper.wasm/README.md#2025-04-11_snippet_1\n\nLANGUAGE: console\nCODE:\n```\npython3 examples/server.py\n```"
    }
  ]
}
