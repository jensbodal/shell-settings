{
  "content": [
    {
      "type": "text",
      "text": "TITLE: Share a Custom Tool to Hugging Face Hub\nDESCRIPTION: This example shows how to publish a custom tool to the Hugging Face Hub using the `push_to_hub` method. It requires a pre-created repository on the Hub and a Hugging Face API token with write access. Tools must be self-contained with imports defined within functions to ensure proper serialization and sharing.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/tutorials/tools.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nmodel_downloads_tool.push_to_hub(\"{your_username}/hf-model-downloads\", token=\"<YOUR_HUGGINGFACEHUB_API_TOKEN>\")\n```\n\n----------------------------------------\n\nTITLE: CodeAgent Python Tool Call Example\nDESCRIPTION: Demonstrates how `CodeAgent` generates tool calls as Python code snippets, showing a simple `search_docs` function call and printing its result. This highlights its code-centric approach, allowing for complex logic and control flow.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/guided_tour.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nresult = search_docs(\"What is the capital of France?\")\nprint(result)\n```\n\n----------------------------------------\n\nTITLE: Implement Custom Webpage Visitor Tool in Python\nDESCRIPTION: Provides the full Python implementation for the `visit_webpage` tool. This function uses `requests` to fetch webpage content and `markdownify` to convert HTML to Markdown, including error handling for network requests.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/examples/multiagents.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport re\nimport requests\nfrom markdownify import markdownify\nfrom requests.exceptions import RequestException\nfrom smolagents import tool\n\n\n@tool\ndef visit_webpage(url: str) -> str:\n    \"\"\"Visits a webpage at the given URL and returns its content as a markdown string.\n\n    Args:\n        url: The URL of the webpage to visit.\n\n    Returns:\n        The content of the webpage converted to Markdown, or an error message if the request fails.\n    \"\"\"\n    try:\n        # Send a GET request to the URL\n        response = requests.get(url)\n        response.raise_for_status()  # Raise an exception for bad status codes\n\n        # Convert the HTML content to Markdown\n        markdown_content = markdownify(response.text).strip()\n\n        # Remove multiple line breaks\n        markdown_content = re.sub(r\"\\n{3,}\", \"\\n\\n\", markdown_content)\n\n        return markdown_content\n\n    except RequestException as e:\n        return f\"Error fetching the webpage: {str(e)}\"\n    except Exception as e:\n        return f\"An unexpected error occurred: {str(e)}\"\n```\n\n----------------------------------------\n\nTITLE: Improved Weather API Tool with Validation and Clear Output\nDESCRIPTION: This snippet presents an improved version of the weather tool. It includes explicit format requirements for 'location' and 'date_time' in its docstring, incorporates robust error handling with informative messages for invalid date formats, and formats the output into a human-readable string. This design significantly reduces the burden on the LLM for understanding and correcting errors.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/tutorials/building_good_agents.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@tool\ndef get_weather_api(location: str, date_time: str) -> str:\n    \"\"\"\n    Returns the weather report.\n\n    Args:\n        location: the name of the place that you want the weather for. Should be a place name, followed by possibly a city name, then a country, like \"Anchor Point, Taghazout, Morocco\".\n        date_time: the date and time for which you want the report, formatted as '%m/%d/%y %H:%M:%S'.\n    \"\"\"\n    lon, lat = convert_location_to_coordinates(location)\n    try:\n        date_time = datetime.strptime(date_time)\n    except Exception as e:\n        raise ValueError(\"Conversion of `date_time` to datetime format failed, make sure to provide a string in format '%m/%d/%y %H:%M:%S'. Full trace:\" + str(e))\n    temperature_celsius, risk_of_rain, wave_height = get_weather_report_at_coordinates((lon, lat), date_time)\n    return f\"Weather report for {location}, {date_time}: Temperature will be {temperature_celsius}°C, risk of rain is {risk_of_rain*100:.0f}%, wave height is {wave_height}m.\"\n```\n\n----------------------------------------\n\nTITLE: Passing Additional Arguments to a SmolAgent\nDESCRIPTION: This example demonstrates how to provide extra context or data to a SmolAgent beyond the primary task string. By using the 'additional_args' parameter in the 'agent.run()' method, users can pass any type of object, such as a URL to an audio file, allowing the agent to leverage diverse inputs for its operations.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/tutorials/building_good_agents.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom smolagents import CodeAgent, InferenceClientModel\n\nmodel_id = \"meta-llama/Llama-3.3-70B-Instruct\"\n\nagent = CodeAgent(tools=[], model=InferenceClientModel(model_id=model_id), add_base_tools=True)\n\nagent.run(\n    \"Why does Mike not know many people in New York?\",\n    additional_args={\"mp3_sound_file_url\":'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/recording.mp3'}\n)\n```\n\n----------------------------------------\n\nTITLE: Define Custom Retriever Tool with SmolAgents\nDESCRIPTION: This Python class `RetrieverTool` extends `smolagents.Tool` to create a custom retrieval mechanism. It uses BM25 for document retrieval, initialized with processed documents, and formats the output for readability. The `forward` method executes the retrieval based on a provided query string.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/examples/rag.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom smolagents import Tool\n\nclass RetrieverTool(Tool):\n    name = \"retriever\"\n    description = \"Uses semantic search to retrieve the parts of transformers documentation that could be most relevant to answer your query.\"\n    inputs = {\n        \"query\": {\n            \"type\": \"string\",\n            \"description\": \"The query to perform. This should be semantically close to your target documents. Use the affirmative form rather than a question.\",\n        }\n    }\n    output_type = \"string\"\n\n    def __init__(self, docs, **kwargs):\n        super().__init__(**kwargs)\n        # Initialize the retriever with our processed documents\n        self.retriever = BM25Retriever.from_documents(\n            docs, k=10  # Return top 10 most relevant documents\n        )\n\n    def forward(self, query: str) -> str:\n        \"\"\"Execute the retrieval based on the provided query.\"\"\"\n        assert isinstance(query, str), \"Your search query must be a string\"\n\n        # Retrieve relevant documents\n        docs = self.retriever.invoke(query)\n\n        # Format the retrieved documents for readability\n        return \"\\nRetrieved documents:\\n\" + \"\".join(\n            [\n                f\"\\n\\n===== Document {str(i)} =====\\n\" + doc.page_content\n                for i, doc in enumerate(docs)\n            ]\n        )\n\n# Initialize our retriever tool with the processed documents\nretriever_tool = RetrieverTool(docs_processed)\n```\n\n----------------------------------------\n\nTITLE: Dynamically Add Tool to SmolAgents Toolbox\nDESCRIPTION: Shows how to manage an agent's toolbox by adding a new tool. This snippet initializes a `CodeAgent` and then directly modifies its `agent.tools` dictionary to include a `model_download_tool`, expanding the agent's capabilities.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/tutorials/tools.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom smolagents import InferenceClientModel\n\nmodel = InferenceClientModel(model_id=\"Qwen/Qwen2.5-Coder-32B-Instruct\")\n\nagent = CodeAgent(tools=[], model=model, add_base_tools=True)\nagent.tools[model_download_tool.name] = model_download_tool\n```\n\n----------------------------------------\n\nTITLE: Initialize CodeAgent with Additional Authorized Imports\nDESCRIPTION: Shows how to initialize a `CodeAgent` instance, providing an empty list of tools, a model, and explicitly authorizing `requests` and `bs4` modules for import within the agent's execution environment. It then demonstrates running a query that would utilize these authorized imports.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/guided_tour.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nmodel = InferenceClientModel()\nagent = CodeAgent(tools=[], model=model, additional_authorized_imports=['requests', 'bs4'])\nagent.run(\"Could you get me the title of the page at url 'https://huggingface.co/blog'?\")\n```\n\n----------------------------------------\n\nTITLE: Define SQL Query Tool for Agent\nDESCRIPTION: This Python function, decorated with `@smolagents.tool`, defines a callable tool named `sql_engine`. It enables the agent to execute SQL queries against the in-memory database and returns the results as a string. The comprehensive docstring provides essential context, including table schema and argument details, for the LLM to effectively use this tool.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/examples/text_to_sql.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom smolagents import tool\n\n@tool\ndef sql_engine(query: str) -> str:\n    \"\"\"\n    Allows you to perform SQL queries on the table. Returns a string representation of the result.\n    The table is named 'receipts'. Its description is as follows:\n        Columns:\n        - receipt_id: INTEGER\n        - customer_name: VARCHAR(16)\n        - price: FLOAT\n        - tip: FLOAT\n\n    Args:\n        query: The query to perform. This should be correct SQL.\n    \"\"\"\n    output = \"\"\n    with engine.connect() as con:\n        rows = con.execute(text(query))\n        for row in rows:\n            output += \"\\n\" + str(row)\n    return output\n```\n\n----------------------------------------\n\nTITLE: Initialize and Run Text-to-SQL Agent\nDESCRIPTION: This snippet initializes a `CodeAgent` from `smolagents`, providing it with the `sql_engine` tool and configuring it to use a specified LLM via `InferenceClientModel`. It then demonstrates running the agent with a natural language query, allowing the agent to generate and execute SQL to retrieve the requested information.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/examples/text_to_sql.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom smolagents import CodeAgent, InferenceClientModel\n\nagent = CodeAgent(\n    tools=[sql_engine],\n    model=InferenceClientModel(model_id=\"meta-llama/Llama-3.1-8B-Instruct\"),\n)\nagent.run(\"Can you give me the name of the client who got the most expensive receipt?\")\n```\n\n----------------------------------------\n\nTITLE: Execute Agent with Newly Added Custom Tool\nDESCRIPTION: Demonstrates running a `CodeAgent` after a new tool has been dynamically added to its toolbox. The agent is prompted with a query that requires the functionality of the newly added tool, showcasing its expanded capabilities.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/tutorials/tools.md#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nagent.run(\n    \"Can you give me the name of the model that has the most downloads in the 'text-to-video' task on the Hugging Face Hub but reverse the letters?\"\n)\n```\n\n----------------------------------------\n\nTITLE: Agent Execution Trace for Image Generation\nDESCRIPTION: This snippet illustrates an agent's execution trace when attempting to generate an image. It demonstrates a common LLM reasoning error where the agent returns a file path instead of the actual image, highlighting the importance of proper output handling and LLM capabilities. The trace shows calls to an `image_generator` function and a `final_answer` function.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/tutorials/building_good_agents.md#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nimage_generator(prompt=\"A cool, futuristic sports car with LED headlights, aerodynamic design, and vibrant color, high-res, photorealistic\")\nfinal_answer(\"/var/folders/6m/9b1tts6d5w960j80wbw9tx3m0000gn/T/tmpx09qfsdd/652f0007-3ee9-44e2-94ac-90dae6bb89a4.png\")\n```\n\n----------------------------------------\n\nTITLE: Initialize OpenRouter Model with OpenAIServerModel\nDESCRIPTION: This Python code demonstrates how to initialize any model available on OpenRouter (e.g., `openai/gpt-4o`) using `smolagents.OpenAIServerModel`. It configures the `api_base` to OpenRouter's API endpoint and uses the provided API key for authentication.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/examples/using_different_models.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom smolagents import OpenAIServerModel\n\nmodel = OpenAIServerModel(\n    # You can use any model ID available on OpenRouter\n    model_id=\"openai/gpt-4o\",\n    # OpenRouter API base URL\n    api_base=\"https://openrouter.ai/api/v1\",\n    api_key=OPENROUTER_API_KEY,\n)\n```\n\n----------------------------------------\n\nTITLE: Initialize CodeAgent with OpenAI or Anthropic API via LiteLLM\nDESCRIPTION: Demonstrates configuring a `CodeAgent` to use OpenAI or Anthropic APIs through `LiteLLMModel`. Authentication requires setting `ANTHROPIC_API_KEY` or `OPENAI_API_KEY` environment variables, or passing the `api_key` parameter directly.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/guided_tour.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# !pip install smolagents[litellm]\nfrom smolagents import CodeAgent, LiteLLMModel\n\nmodel = LiteLLMModel(model_id=\"anthropic/claude-3-5-sonnet-latest\", api_key=\"YOUR_ANTHROPIC_API_KEY\") # Could use 'gpt-4o'\nagent = CodeAgent(tools=[], model=model, add_base_tools=True)\n\nagent.run(\n    \"Could you give me the 118th number in the Fibonacci sequence?\",\n)\n```\n\n----------------------------------------\n\nTITLE: Integrate LangChain Tools with SmolAgents\nDESCRIPTION: Explains how to incorporate tools from the LangChain ecosystem into SmolAgents using `Tool.from_langchain`. This example demonstrates importing a `serpapi` search tool, noting the necessary `langchain` and `google-search-results` pip installations.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/tutorials/tools.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain.agents import load_tools\n\nsearch_tool = Tool.from_langchain(load_tools([\"serpapi\"])[0])\n\nagent = CodeAgent(tools=[search_tool], model=model)\n\nagent.run(\"How many more blocks (also denoted as layers) are in BERT base encoder compared to the encoder from the architecture proposed in Attention is All You Need?\")\n```\n\n----------------------------------------\n\nTITLE: Initialize and Run ToolCallingAgent\nDESCRIPTION: Demonstrates how to initialize a `ToolCallingAgent` with an empty tool list and a specified model, then execute a query to perform a tool call.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/guided_tour.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom smolagents import ToolCallingAgent\n\nagent = ToolCallingAgent(tools=[], model=model)\nagent.run(\"Could you get me the title of the page at url 'https://huggingface.co/blog'?\")\n```\n\n----------------------------------------\n\nTITLE: Integrate and Run Custom Tool with Hugging Face CodeAgent\nDESCRIPTION: This Python code demonstrates the integration of a newly created custom tool (e.g., `model_download_tool`) into a `smolagents.CodeAgent`. It shows how to initialize the agent with a list of tools and an `InferenceClientModel`, then execute a natural language query, allowing the agent to autonomously select and utilize the appropriate custom tool to fulfill the request.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/guided_tour.md#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nfrom smolagents import CodeAgent, InferenceClientModel\nagent = CodeAgent(tools=[model_download_tool], model=InferenceClientModel())\nagent.run(\n    \"Can you give me the name of the model that has the most downloads in the 'text-to-video' task on the Hugging Face Hub?\"\n)\n```\n\n----------------------------------------\n\nTITLE: Configure Inference Model ID for Agent\nDESCRIPTION: Sets the `model_id` variable to specify the large language model (Qwen/Qwen2.5-Coder-32B-Instruct) that will power the agent. This model is accessed through Hugging Face's Inference API.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/examples/multiagents.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmodel_id = \"Qwen/Qwen2.5-Coder-32B-Instruct\"\n```\n\n----------------------------------------\n\nTITLE: Integrate Ollama with Smolagents using LiteLLMModel\nDESCRIPTION: This snippet demonstrates how to connect `smolagents` to a local Ollama instance. It utilizes `LiteLLMModel` and requires specifying the `model_id`, `api_base` (typically `http://localhost:11434`), and adjusting `num_ctx` for better performance. The example shows an agent running a simple query.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/guided_tour.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# !pip install smolagents[litellm]\nfrom smolagents import CodeAgent, LiteLLMModel\n\nmodel = LiteLLMModel(\n    model_id=\"ollama_chat/llama3.2\", # This model is a bit weak for agentic behaviours though\n    api_base=\"http://localhost:11434\", # replace with 127.0.0.1:11434 or remote open-ai compatible server if necessary\n    api_key=\"YOUR_API_KEY\", # replace with API key if necessary\n    num_ctx=8192 # ollama default is 2048 which will fail horribly. 8192 works for easy tasks, more is better. Check https://huggingface.co/spaces/NyxKrage/LLM-Model-VRAM-Calculator to calculate how much VRAM this will need for the selected model.\n)\n\nagent = CodeAgent(tools=[], model=model, add_base_tools=True)\n\nagent.run(\n    \"Could you give me the 118th number in the Fibonacci sequence?\",\n)\n```\n\n----------------------------------------\n\nTITLE: Define Helium Instructions String for Agent\nDESCRIPTION: Assigns a multi-line string containing detailed instructions and embedded code examples for using Helium to a Python variable, which is then passed to the agent for context.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/examples/web_browser.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nhelium_instructions = \"\"\"\nYou can use helium to access websites. Don't bother about the helium driver, it's already managed.\nWe've already ran \"from helium import *\"\nThen you can go to pages!\nCode:\n```py\ngo_to('github.com/trending')\n```<end_code>\n\nYou can directly click clickable elements by inputting the text that appears on them.\nCode:\n```py\nclick(\"Top products\")\n```<end_code>\n\nIf it's a link:\nCode:\n```py\nclick(Link(\"Top products\"))\n```<end_code>\n\nIf you try to interact with an element and it's not found, you'll get a LookupError.\nIn general stop your action after each button click to see what happens on your screenshot.\nNever try to login in a page.\n\nTo scroll up or down, use scroll_down or scroll_up with as an argument the number of pixels to scroll from.\nCode:\n```py\nscroll_down(num_pixels=1200) # This will scroll one viewport down\n```<end_code>\n\nWhen you have pop-ups with a cross icon to close, don't try to click the close icon by finding its element or targeting an 'X' element (this most often fails).\nJust use your built-in tool `close_popups` to close them:\nCode:\n```py\nclose_popups()\n```<end_code>\n\nYou can use .exists() to check for the existence of an element. For example:\nCode:\n```py\nif Text('Accept cookies?').exists():\n    click('I accept')\n```<end_code>\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Initialize Web Automation Agent\nDESCRIPTION: Initializes the `CodeAgent` from `smolagents` with the previously defined browser interaction tools. It configures the agent to use a specified Vision-Language Model (VLM), authorizes additional imports like `helium`, and integrates the `save_screenshot` callback for visual observations. The agent's behavior is further controlled by `max_steps` and `verbosity_level`.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/examples/web_browser.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom smolagents import InferenceClientModel\n\n# Initialize the model\nmodel_id = \"Qwen/Qwen2-VL-72B-Instruct\"  # You can change this to your preferred VLM model\nmodel = InferenceClientModel(model_id=model_id)\n\n# Create the agent\nagent = CodeAgent(\n    tools=[go_back, close_popups, search_item_ctrl_f],\n    model=model,\n    additional_authorized_imports=[\"helium\"],\n    step_callbacks=[save_screenshot],\n    max_steps=20,\n    verbosity_level=2,\n)\n```\n\n----------------------------------------\n\nTITLE: Configure Chrome Browser and Screenshot Callback\nDESCRIPTION: Sets up Chrome browser options for `selenium` to control window size, position, and disable PDF viewer. It then initializes the browser using `helium` and defines a `save_screenshot` callback function. This callback captures and processes browser screenshots after each agent step, updating observations with the current URL and managing memory by clearing old screenshots.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/examples/web_browser.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Configure Chrome options\nchrome_options = webdriver.ChromeOptions()\nchrome_options.add_argument(\"--force-device-scale-factor=1\")\nchrome_options.add_argument(\"--window-size=1000,1350\")\nchrome_options.add_argument(\"--disable-pdf-viewer\")\nchrome_options.add_argument(\"--window-position=0,0\")\n\n# Initialize the browser\ndriver = helium.start_chrome(headless=False, options=chrome_options)\n\n# Set up screenshot callback\ndef save_screenshot(memory_step: ActionStep, agent: CodeAgent) -> None:\n    sleep(1.0)  # Let JavaScript animations happen before taking the screenshot\n    driver = helium.get_driver()\n    current_step = memory_step.step_number\n    if driver is not None:\n        for previous_memory_step in agent.memory.steps:  # Remove previous screenshots for lean processing\n            if isinstance(previous_memory_step, ActionStep) and previous_memory_step.step_number <= current_step - 2:\n                previous_memory_step.observations_images = None\n        png_bytes = driver.get_screenshot_as_png()\n        image = Image.open(BytesIO(png_bytes))\n        print(f\"Captured a browser screenshot: {image.size} pixels\")\n        memory_step.observations_images = [image.copy()]  # Create a copy to ensure it persists\n\n    # Update observations with current URL\n    url_info = f\"Current url: {driver.current_url}\"\n    memory_step.observations = (\n        url_info if memory_step.observations is None else memory_step.observations + \"\\n\" + url_info\n    )\n```\n\n----------------------------------------\n\nTITLE: smolagents.Tool Class API Reference\nDESCRIPTION: Detailed API documentation for the `smolagents.Tool` base class, outlining its attributes and methods. This class serves as the foundation for creating custom tools that can be integrated into LLM-powered agents, enabling them to perform specific actions.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/tutorials/tools.md#_snippet_3\n\nLANGUAGE: APIDOC\nCODE:\n```\nclass smolagents.Tool:\n  Description: Base class for defining custom tools that LLMs can use within an agentic system.\n  \n  Attributes:\n    name: str\n      Description: The unique name of the tool, typically describing its primary action.\n    description: str\n      Description: A detailed, human-readable description of the tool's purpose and functionality. This is used to inform the LLM.\n    inputs: dict\n      Description: A dictionary specifying the input parameters required by the tool's 'forward' method. Each key represents a parameter name, and its value is a dictionary with 'type' (Pydantic format) and 'description'.\n    output_type: str\n      Description: The expected data type of the output returned by the tool's 'forward' method (Pydantic format).\n  \n  Methods:\n    __init__(self):\n      Description: Constructor for the Tool class. When subclassing, it's recommended that this method accepts no arguments other than 'self' to ensure proper serialization and sharing to the Hugging Face Hub.\n    \n    forward(self, *args, **kwargs) -> Any:\n      Description: The core method containing the tool's execution logic. All necessary imports for this method should be defined directly within its scope to ensure self-containment.\n      Parameters:\n        *args, **kwargs: The input arguments corresponding to the 'inputs' attribute.\n      Returns:\n        Any: The result of the tool's operation, conforming to the 'output_type'.\n    \n    push_to_hub(self, repo_id: str, token: str):\n      Description: Publishes the custom tool to the Hugging Face Hub as a Space repository. Requires a pre-existing repository and a Hugging Face API token with write access.\n      Parameters:\n        repo_id: str\n          The target repository ID on the Hugging Face Hub (e.g., \"{your_username}/my-tool\").\n        token: str\n          Your Hugging Face API token with write permissions.\n    \n    save(self, path: str):\n      Description: Saves the tool's definition and associated code to a local path. This method is implicitly used during the push_to_hub process.\n      Parameters:\n        path: str\n          The local file path where the tool should be saved.\n  \n  Static Methods / Functions:\n    load_tool(repo_id: str, trust_remote_code: bool) -> Tool:\n      Description: Loads a tool instance directly from a specified Hugging Face Hub repository.\n      Parameters:\n        repo_id: str\n          The repository ID of the tool on the Hugging Face Hub.\n        trust_remote_code: bool\n          A boolean flag that must be set to `True` to allow loading and executing custom code from a remote repository. Use with caution.\n      Returns:\n        Tool: An instantiated object of the loaded tool.\n    \n    Tool.from_hub(cls, repo_id: str, trust_remote_code: bool) -> Tool:\n      Description: A class method alternative to `load_tool` for creating a Tool instance by loading it from the Hugging Face Hub.\n      Parameters:\n        repo_id: str\n          The repository ID of the tool on the Hugging Face Hub.\n        trust_remote_code: bool\n          A boolean flag that must be set to `True` to allow loading and executing custom code from a remote repository. Use with caution.\n      Returns:\n        Tool: An instantiated object of the loaded tool.\n```\n\n----------------------------------------\n\nTITLE: Run Agent to Answer Questions\nDESCRIPTION: This Python code demonstrates how to use the initialized `CodeAgent` to answer a specific question. It defines a question string that requires information retrieval and then calls the agent's `run` method with the question. Finally, it prints the agent's generated output, which includes the final answer.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/examples/rag.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Ask a question that requires retrieving information\nquestion = \"For a transformers model training, which is slower, the forward or the backward pass?\"\n\n# Run the agent to get an answer\nagent_output = agent.run(question)\n\n# Display the final answer\nprint(\"\\nFinal answer:\")\nprint(agent_output)\n```\n\n----------------------------------------\n\nTITLE: Execute an Agent with a Specific Task\nDESCRIPTION: Shows a straightforward example of running an initialized agent with a given natural language prompt. The agent will attempt to process the request based on its configuration and available tools.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/guided_tour.md#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nagent.run(\"Calculate the least common multiple of 3 and 7\")\n```\n\n----------------------------------------\n\nTITLE: Calculate Pope's Age Raised to a Power\nDESCRIPTION: Shows how to retrieve information (Pope's age) using `wiki` and `web_search` tools, then perform a mathematical computation on the retrieved data before providing the final answer.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/tutorials/building_good_agents.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\npope_age_wiki = wiki(query=\"current pope age\")\nprint(\"Pope age as per wikipedia:\", pope_age_wiki)\npope_age_search = web_search(query=\"current pope age\")\nprint(\"Pope age as per google search:\", pope_age_search)\n```\n\nLANGUAGE: python\nCODE:\n```\npope_current_age = 88 ** 0.36\nfinal_answer(pope_current_age)\n```\n\n----------------------------------------\n\nTITLE: Helium Web Automation Usage Examples\nDESCRIPTION: Illustrates common Helium functions for web automation, including navigating to URLs, clicking various types of elements, scrolling, closing pop-ups, and conditionally interacting with elements based on their existence.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/examples/web_browser.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ngo_to('github.com/trending')\n```\n\nLANGUAGE: python\nCODE:\n```\nclick(\"Top products\")\n```\n\nLANGUAGE: python\nCODE:\n```\nclick(Link(\"Top products\"))\n```\n\nLANGUAGE: python\nCODE:\n```\nscroll_down(num_pixels=1200) # This will scroll one viewport down\n```\n\nLANGUAGE: python\nCODE:\n```\nclose_popups()\n```\n\nLANGUAGE: python\nCODE:\n```\nif Text('Accept cookies?').exists():\n    click('I accept')\n```\n\n----------------------------------------\n\nTITLE: Configure and Run CodeAgent with Multi-Table SQL Tool\nDESCRIPTION: This Python snippet configures the `CodeAgent` with the previously updated SQL tool description and switches to a more powerful LLM model ('Qwen/Qwen2.5-Coder-32B-Instruct') to handle complex text-to-SQL queries involving multiple tables. It then executes a query to find which waiter earned more tips, demonstrating the agent's ability to reason over joined data.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/examples/text_to_sql.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nsql_engine.description = updated_description\n\nagent = CodeAgent(\n    tools=[sql_engine],\n    model=InferenceClientModel(model_id=\"Qwen/Qwen2.5-Coder-32B-Instruct\"),\n)\n\nagent.run(\"Which waiter got more total money from tips?\")\n```\n\n----------------------------------------\n\nTITLE: Connect to Azure OpenAI with AzureOpenAIServerModel\nDESCRIPTION: This example illustrates how to integrate `smolagents` with Azure OpenAI using the `AzureOpenAIServerModel`. Users need to provide their model deployment name as `model_id` and ensure `azure_endpoint`, `api_key`, and `api_version` are either passed as arguments or set as environment variables. The agent then executes a query.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/guided_tour.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# !pip install smolagents[openai]\nfrom smolagents import CodeAgent, AzureOpenAIServerModel\n\nmodel = AzureOpenAIServerModel(model_id=\"gpt-4o-mini\")\nagent = CodeAgent(tools=[], model=model, add_base_tools=True)\n\nagent.run(\n    \"Could you give me the 118th number in the Fibonacci sequence?\",\n)\n```\n\n----------------------------------------\n\nTITLE: Basic Amazon Bedrock Integration with AmazonBedrockServerModel\nDESCRIPTION: This snippet shows the basic setup for integrating `smolagents` with Amazon Bedrock using `AmazonBedrockServerModel`. It requires specifying the `model_id` for the desired Bedrock model. The example initializes a `CodeAgent` and runs a simple query.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/guided_tour.md#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# !pip install smolagents[aws_sdk]\nfrom smolagents import CodeAgent, AmazonBedrockServerModel\n\nmodel = AmazonBedrockServerModel(model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\")\nagent = CodeAgent(tools=[], model=model, add_base_tools=True)\n\nagent.run(\n    \"Could you give me the 118th number in the Fibonacci sequence?\",\n)\n```\n\n----------------------------------------\n\nTITLE: Configure CodeAgent with various LLM providers\nDESCRIPTION: Illustrates the flexibility of `smolagents` in integrating with different large language models. Examples include using a specific Hugging Face model via `InferenceClientModel`, OpenAI/Anthropic models via `LiteLLMModel` (requiring `smolagents[litellm]`), and local models via `TransformersModel` (requiring `smolagents[transformers]`).\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/index.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Using a specific model from Hugging Face\nmodel = InferenceClientModel(model_id=\"meta-llama/Llama-2-70b-chat-hf\")\n\n# Using OpenAI/Anthropic (requires smolagents[litellm])\nfrom smolagents import LiteLLMModel\nmodel = LiteLLMModel(model_id=\"gpt-4\")\n\n# Using local models (requires smolagents[transformers])\nfrom smolagents import TransformersModel\nmodel = TransformersModel(model_id=\"meta-llama/Llama-2-7b-chat-hf\")\n```\n\n----------------------------------------\n\nTITLE: Initialize Advanced Retrieval Agent with SmolAgents\nDESCRIPTION: This Python code initializes a `CodeAgent` from `smolagents`, providing it with a list of available tools, such as the `retriever_tool`. It configures the agent with an `InferenceClientModel` (defaulting to 'Qwen/Qwen2.5-Coder-32B-Instruct'), sets a maximum number of reasoning steps, and specifies the verbosity level for detailed agent reasoning. It also shows how to specify a custom model ID.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/examples/rag.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom smolagents import InferenceClientModel, CodeAgent\n\n# Initialize the agent with our retriever tool\nagent = CodeAgent(\n    tools=[retriever_tool],  # List of tools available to the agent\n    model=InferenceClientModel(),  # Default model \"Qwen/Qwen2.5-Coder-32B-Instruct\"\n    max_steps=4,  # Limit the number of reasoning steps\n    verbosity_level=2  # Show detailed agent reasoning\n)\n\n# To use a specific model, you can specify it like this:\n# model=InferenceClientModel(model_id=\"meta-llama/Llama-3.3-70B-Instruct\")\n```\n\n----------------------------------------\n\nTITLE: Create Agent Tool with @tool Decorator in Python\nDESCRIPTION: This Python example illustrates how to define a custom agent tool by wrapping a function with the `@tool` decorator from `smolagents`. It emphasizes the importance of clear function names, precise type hints for inputs and outputs, and a detailed docstring with an `Args:` section to guide the LLM's understanding and usage of the tool.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/guided_tour.md#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nfrom smolagents import tool\n\n@tool\ndef model_download_tool(task: str) -> str:\n    \"\"\"\n    This is a tool that returns the most downloaded model of a given task on the Hugging Face Hub.\n    It returns the name of the checkpoint.\n\n    Args:\n        task: The task for which to get the download count.\n    \"\"\"\n    most_downloaded_model = next(iter(list_models(filter=task, sort=\"downloads\", direction=-1)))\n    return most_downloaded_model.id\n```\n\n----------------------------------------\n\nTITLE: Poorly Designed Weather API Tool Example\nDESCRIPTION: This code snippet demonstrates a poorly designed tool for retrieving weather data. It lacks clear input format specifications for 'date_time' and 'location', provides no detailed error logging, and returns an output that is difficult for an LLM to parse or understand, making it hard to debug or use effectively.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/tutorials/building_good_agents.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport datetime\nfrom smolagents import tool\n\ndef get_weather_report_at_coordinates(coordinates, date_time):\n    # Dummy function, returns a list of [temperature in °C, risk of rain on a scale 0-1, wave height in m]\n    return [28.0, 0.35, 0.85]\n\ndef convert_location_to_coordinates(location):\n    # Returns dummy coordinates\n    return [3.3, -42.0]\n\n@tool\ndef get_weather_api(location: str, date_time: str) -> str:\n    \"\"\"\n    Returns the weather report.\n\n    Args:\n        location: the name of the place that you want the weather for.\n        date_time: the date and time for which you want the report.\n    \"\"\"\n    lon, lat = convert_location_to_coordinates(location)\n    date_time = datetime.strptime(date_time)\n    return str(get_weather_report_at_coordinates((lon, lat), date_time))\n```\n\n----------------------------------------\n\nTITLE: Import Hugging Face Gradio Space as SmolAgents Tool\nDESCRIPTION: Demonstrates how to integrate a Hugging Face Gradio Space as a tool within SmolAgents using `Tool.from_space`. It requires the Space ID, a name, and a description, leveraging the `gradio-client` library for underlying calls.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/tutorials/tools.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimage_generation_tool = Tool.from_space(\n    \"black-forest-labs/FLUX.1-schnell\",\n    name=\"image_generator\",\n    description=\"Generate an image from a prompt\"\n)\n\nimage_generation_tool(\"A sunny beach\")\n```\n\n----------------------------------------\n\nTITLE: Modify Agent System Prompt Template\nDESCRIPTION: This snippet demonstrates how to programmatically append additional text to an agent's existing system prompt template. This allows for dynamic modification of the agent's core instructions or context after initialization.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/tutorials/building_good_agents.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nagent.prompt_templates[\"system_prompt\"] = agent.prompt_templates[\"system_prompt\"] + \"\\nHere you go!\"\n```\n\n----------------------------------------\n\nTITLE: ToolCallingAgent / CodeAgent Initialization Parameters\nDESCRIPTION: Defines the constructor parameters for `ToolCallingAgent` and `CodeAgent`, detailing the various `AgentModel` types supported for powering the agent and how to configure tools.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/guided_tour.md#_snippet_6\n\nLANGUAGE: APIDOC\nCODE:\n```\nToolCallingAgent / CodeAgent:\n  __init__(model: AgentModel, tools: list[Tool], add_base_tools: bool = False)\n\n  Parameters:\n    model (AgentModel): A text-generation model to power your agent.\n      - TransformersModel: Takes a pre-initialized `transformers` pipeline to run inference on your local machine.\n      - InferenceClientModel: Leverages `huggingface_hub.InferenceClient` and supports all Hugging Face Inference Providers.\n        - Parameters: model_id (str), token (str, optional), provider (str, optional)\n      - LiteLLMModel: Allows calling 100+ different models and providers through LiteLLM.\n        - Parameters: model_id (str), api_key (str, optional)\n      - AzureOpenAIServerModel: Allows using OpenAI models deployed in Azure.\n      - AmazonBedrockServerModel: Allows using Amazon Bedrock in AWS.\n      - MLXModel: Creates an `mlx-lm` pipeline to run inference on your local machine.\n\n    tools (list[Tool]): A list of `Tools` that the agent can use to solve the task. Can be an empty list.\n    add_base_tools (bool, optional): If True, adds the default toolbox on top of your `tools` list. Defaults to False.\n```\n\n----------------------------------------\n\nTITLE: Install Smolagents Core Library\nDESCRIPTION: Installs the fundamental `smolagents` library without any optional dependencies. This provides the core functionalities of the framework.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/installation.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install smolagents\n```\n\n----------------------------------------\n\nTITLE: Install smolagents dependencies for OpenAI-compatible models\nDESCRIPTION: This command installs the necessary `smolagents` dependencies, specifically the `openai` extra, which is required for interacting with various OpenAI-compatible APIs, including Google Gemini and OpenRouter.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/examples/using_different_models.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install smolagents[openai]\n```\n\n----------------------------------------\n\nTITLE: tool Function API Reference\nDESCRIPTION: API reference for the `tool` function, likely a decorator or utility function for defining and registering tools in smolagents.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/reference/tools.md#_snippet_1\n\nLANGUAGE: APIDOC\nCODE:\n```\ndef tool(*args, **kwargs)\n```\n\n----------------------------------------\n\nTITLE: Generate Table Schema Description for LLM\nDESCRIPTION: This Python snippet uses SQLAlchemy's inspector to dynamically retrieve column names and types from the 'receipts' table. It then formats this information into a descriptive string, which will be embedded in the LLM's prompt to inform it about the table structure.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/examples/text_to_sql.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ninspector = inspect(engine)\ncolumns_info = [(col[\"name\"], col[\"type\"]) for col in inspector.get_columns(\"receipts\")]\n\ntable_description = \"Columns:\\n\" + \"\\n\".join([f\"  - {name}: {col_type}\" for name, col_type in columns_info])\nprint(table_description)\n```\n\n----------------------------------------\n\nTITLE: Initialize Agent with Helium Library Import\nDESCRIPTION: Executes a Python command within the agent's environment to import the Helium library, making its functions available for subsequent web automation tasks.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/examples/web_browser.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nagent.python_executor(\"from helium import *\", agent.state)\n```\n\n----------------------------------------\n\nTITLE: WebSearchTool Class API Reference\nDESCRIPTION: API reference for the `WebSearchTool` class, a generic default tool designed for performing broad web searches.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/reference/tools.md#_snippet_7\n\nLANGUAGE: APIDOC\nCODE:\n```\nclass WebSearchTool:\n```\n\n----------------------------------------\n\nTITLE: Demonstrating LocalPythonExecutor Security Safeguards\nDESCRIPTION: This Python code snippet illustrates how `smolagents`' `LocalPythonExecutor` prevents various unsafe operations. It initializes the executor with authorized imports, then attempts to run commands that are either syntactically invalid, try to import unauthorized modules (like `os`), access forbidden submodules (`random._os`), or create infinite loops. The output demonstrates how the executor catches and reports these security violations, ensuring a safer execution environment.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/tutorials/secure_code_execution.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom smolagents.local_python_executor import LocalPythonExecutor\n\n# Set up custom executor, authorize package \"numpy\"\ncustom_executor = LocalPythonExecutor([\"numpy\"])\n\n# Utilisty for pretty printing errors\ndef run_capture_exception(command: str):\n    try:\n        custom_executor(harmful_command)\n    except Exception as e:\n        print(\"ERROR:\\n\", e)\n\n# Undefined command just do not work\nharmful_command=\"!echo Bad command\"\nrun_capture_exception(harmful_command)\n# >>> ERROR: invalid syntax (<unknown>, line 1)\n\n\n# Imports like os will not be performed unless explicitly added to `additional_authorized_imports`\nharmful_command=\"import os; exit_code = os.system(\"echo Bad command\")\"\nrun_capture_exception(harmful_command)\n# >>> ERROR: Code execution failed at line 'import os' due to: InterpreterError: Import of os is not allowed. Authorized imports are: ['statistics', 'numpy', 'itertools', 'time', 'queue', 'collections', 'math', 'random', 're', 'datetime', 'stat', 'unicodedata']\n\n# Even in authorized imports, potentially harmful packages will not be imported\nharmful_command=\"import random; random._os.system('echo Bad command')\"\nrun_capture_exception(harmful_command)\n# >>> ERROR: Code execution failed at line 'random._os.system('echo Bad command')' due to: InterpreterError: Forbidden access to module: os\n\n# Infinite loop are interrupted after N operations\nharmful_command=\"\"\"\nwhile True:\n    pass\n\"\"\"\nrun_capture_exception(harmful_command)\n# >>> ERROR: Code execution failed at line 'while True: pass' due to: InterpreterError: Maximum number of 1000000 iterations in While loop exceeded\n```\n\n----------------------------------------\n\nTITLE: CodeAgent Class API Reference\nDESCRIPTION: API documentation for the `CodeAgent` class, a specialized agent that generates its tool calls in Python code. This is the default agent type. It requires a language model (`model`) and a list of available `tools` for initialization.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/reference/agents.md#_snippet_1\n\nLANGUAGE: APIDOC\nCODE:\n```\nCodeAgent:\n  __init__(model: Any, tools: List[Any])\n```\n\n----------------------------------------\n\nTITLE: Agent Task: Wikipedia Information Extraction\nDESCRIPTION: Demonstrates how to run the agent with a specific task to navigate to a Wikipedia page and extract a sentence containing particular keywords, combining the search request with the Helium instructions.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/examples/web_browser.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nsearch_request = \"\"\"\nPlease navigate to https://en.wikipedia.org/wiki/Chicago and give me a sentence containing the word \"1992\" that mentions a construction accident.\n\"\"\"\n\nagent_output = agent.run(search_request + helium_instructions)\nprint(\"Final output:\")\nprint(agent_output)\n```\n\n----------------------------------------\n\nTITLE: ToolCollection Class API Reference\nDESCRIPTION: API reference for the `ToolCollection` class, likely used to manage, group, and organize multiple tools within the smolagents ecosystem.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/reference/tools.md#_snippet_12\n\nLANGUAGE: APIDOC\nCODE:\n```\nclass ToolCollection:\n```\n\n----------------------------------------\n\nTITLE: DuckDuckGoSearchTool Class API Reference\nDESCRIPTION: API reference for the `DuckDuckGoSearchTool` class, a default tool specifically configured to perform web searches using the DuckDuckGo search engine.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/reference/tools.md#_snippet_8\n\nLANGUAGE: APIDOC\nCODE:\n```\nclass DuckDuckGoSearchTool:\n```\n\n----------------------------------------\n\nTITLE: Connect to Stdio-based MCP Server using Context Manager\nDESCRIPTION: This snippet demonstrates how to connect to a stdio-based MCP server using `MCPClient` as a Python context manager. It configures `StdioServerParameters` to launch an `uvx` command with specific arguments and environment variables, ensuring dependencies are available. The loaded tools are then passed to a `CodeAgent` to execute a research query.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/tutorials/tools.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom smolagents import MCPClient, CodeAgent\nfrom mcp import StdioServerParameters\nimport os\n\nserver_parameters = StdioServerParameters(\n    command=\"uvx\",  # Using uvx ensures dependencies are available\n    args=[\"--quiet\", \"pubmedmcp@0.1.3\"],\n    env={\"UV_PYTHON\": \"3.12\", **os.environ},\n)\n\nwith MCPClient(server_parameters) as tools:\n    agent = CodeAgent(tools=tools, model=model, add_base_tools=True)\n    agent.run(\"Please find the latest research on COVID-19 treatment.\")\n```\n\n----------------------------------------\n\nTITLE: load_tool Function API Reference\nDESCRIPTION: API reference for the `load_tool` function, used to dynamically load specific tools within the smolagents framework.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/reference/tools.md#_snippet_0\n\nLANGUAGE: APIDOC\nCODE:\n```\ndef load_tool(*args, **kwargs)\n```\n\n----------------------------------------\n\nTITLE: Connect to Streamable HTTP-based MCP Server using Context Manager\nDESCRIPTION: This example illustrates connecting to a streamable HTTP-based MCP server using `MCPClient` as a context manager. It specifies the server's URL and transport type directly within the `MCPClient` initialization. The acquired tools are subsequently used by a `CodeAgent` to process a user query.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/tutorials/tools.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom smolagents import MCPClient, CodeAgent\n\nwith MCPClient({\"url\": \"http://127.0.0.1:8000/mcp\", \"transport\": \"streamable-http\"}) as tools:\n    agent = CodeAgent(tools=tools, model=model, add_base_tools=True)\n    agent.run(\"Please find a remedy for hangover.\")\n```\n\n----------------------------------------\n\nTITLE: Dynamically Update SQL Tool Description for LLM\nDESCRIPTION: This Python code dynamically generates an updated description for the `SQLExecutorTool` by inspecting the database schema. It iterates through specified tables ('receipts', 'waiters') to include their column names and types, ensuring the LLM has accurate and comprehensive information for SQL query generation across joined tables.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/examples/text_to_sql.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nupdated_description = \"\"\"Allows you to perform SQL queries on the table. Beware that this tool's output is a string representation of the execution output.\nIt can use the following tables:\"\"\"\n\ninspector = inspect(engine)\nfor table in [\"receipts\", \"waiters\"]:\n    columns_info = [(col[\"name\"], col[\"type\"]) for col in inspector.get_columns(table)]\n\n    table_description = f\"Table '{table}':\\n\"\n\n    table_description += \"Columns:\\n\" + \"\\n\".join([f\"  - {name}: {col_type}\" for name, col_type in columns_info])\n    updated_description += \"\\n\\n\" + table_description\n\nprint(updated_description)\n```\n\n----------------------------------------\n\nTITLE: Load a Custom Tool from Hugging Face Hub\nDESCRIPTION: This snippet demonstrates how to load a custom tool that has been shared on the Hugging Face Hub. It uses the `smolagents.load_tool` function, which requires specifying the repository ID and setting `trust_remote_code=True` due to the execution of custom code from a remote source. This allows agents to utilize community-contributed or privately shared tools.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/tutorials/tools.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom smolagents import load_tool, CodeAgent\n\nmodel_download_tool = load_tool(\n    \"{your_username}/hf-model-downloads\",\n    trust_remote_code=True\n)\n```\n\n----------------------------------------\n\nTITLE: Manually Use a Web Search Tool\nDESCRIPTION: Illustrates how to directly import, initialize, and invoke a 'WebSearchTool' outside of an agent's context. This demonstrates the standalone usability of individual tools.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/guided_tour.md#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\n# !pip install smolagents[toolkit]\nfrom smolagents import WebSearchTool\n\nsearch_tool = WebSearchTool()\nprint(search_tool(\"Who's the current president of Russia?\"))\n```\n\n----------------------------------------\n\nTITLE: Agent Run Inspection Attributes\nDESCRIPTION: Describes key attributes available on an agent object after a run, allowing for inspection of the execution process. This includes fine-grained logs and a method to convert memory into chat messages for a higher-level view.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/guided_tour.md#_snippet_20\n\nLANGUAGE: APIDOC\nCODE:\n```\nagent.logs: List[Dict]\n  - Stores fine-grained logs of the agent's run.\n  - Each step's information is stored in a dictionary and appended to this list.\n\nagent.write_memory_to_messages(): List[Dict]\n  - Writes the agent's memory as a list of chat messages.\n  - Iterates over log steps, storing relevant information (e.g., system prompt, task, LLM output, tool call output) as messages.\n  - Provides a higher-level view of the run, but not all log details are transcribed.\n```\n\n----------------------------------------\n\nTITLE: Use LiteLLMModel for Amazon Bedrock Integration\nDESCRIPTION: This snippet illustrates how to integrate `smolagents` with Amazon Bedrock models using `LiteLLMModel`. It requires specifying the `model_name` with a `bedrock/` prefix, followed by the specific Bedrock model identifier. This provides an alternative to `AmazonBedrockServerModel` for Bedrock integration.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/guided_tour.md#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom smolagents import LiteLLMModel, CodeAgent\n\nmodel = LiteLLMModel(model_name=\"bedrock/anthropic.claude-3-sonnet-20240229-v1:0\")\nagent = CodeAgent(tools=[], model=model)\n\nagent.run(\"Explain the concept of quantum computing\")\n```"
    }
  ]
}
