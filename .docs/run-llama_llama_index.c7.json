{
  "content": [
    {
      "type": "text",
      "text": "TITLE: Querying Data with LlamaIndex 5-Line Quickstart\nDESCRIPTION: This Python snippet demonstrates the LlamaIndex 5-line quickstart for ingesting and querying data. It loads documents from a 'data' directory, creates a vector store index, initializes a query engine, and then queries the indexed data, printing the response. This example showcases the high-level API for rapid prototyping.\nSOURCE: https://github.com/run-llama/llama_index/blob/main/docs/docs/index.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n\ndocuments = SimpleDirectoryReader(\"data\").load_data()\nindex = VectorStoreIndex.from_documents(documents)\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"Some question about the data should go here\")\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Installing LlamaIndex Core Library - Python\nDESCRIPTION: Installs the core `llama-index` library. This is essential for using LlamaIndex functionalities, especially when running in environments like Google Colab.\nSOURCE: https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/data_connectors/NotionDemo.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n!pip install llama-index\n```\n\n----------------------------------------\n\nTITLE: Installing LlamaIndex Python Package\nDESCRIPTION: Installs or upgrades the LlamaIndex library, a core dependency for building RAG applications, ensuring the latest features and bug fixes are available.\nSOURCE: https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/workflow/rag.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install -U llama-index\n```\n\n----------------------------------------\n\nTITLE: Installing LlamaIndex Core Library - Python\nDESCRIPTION: This command installs the main `llama-index` library, which provides the core functionalities for data indexing, querying, and evaluation. It is a fundamental dependency for running any LlamaIndex-based application or notebook.\nSOURCE: https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/evaluation/correctness_eval.ipynb#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n!pip install llama-index\n```\n\n----------------------------------------\n\nTITLE: Installing LlamaIndex Core Library\nDESCRIPTION: Installs the main `llama-index` library, a core dependency for leveraging LlamaIndex functionalities. This step is particularly important for environments like Google Colab where the library might not be pre-installed.\nSOURCE: https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/llm/xinference_local_deployment.ipynb#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n!pip install llama-index\n```\n\n----------------------------------------\n\nTITLE: Installing LlamaIndex Core Library (Python)\nDESCRIPTION: Installs the core `llama-index` library, which is essential for building and managing indexes, loading documents, and performing queries. This is a fundamental dependency for any LlamaIndex application.\nSOURCE: https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/vector_stores/FaissIndexDemo.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n!pip install llama-index\n```\n\n----------------------------------------\n\nTITLE: Installing LlamaIndex Core Library\nDESCRIPTION: Installs the core `llama-index` library, which is essential for running LlamaIndex applications. This is a fundamental dependency for the examples provided.\nSOURCE: https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/llm/litellm.ipynb#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n!pip install llama-index\n```\n\n----------------------------------------\n\nTITLE: Quickstart Installation of LlamaIndex via Pip\nDESCRIPTION: This command installs the default LlamaIndex starter bundle, which includes core components and common integrations like OpenAI LLMs and embeddings. It's the recommended way to quickly get started with LlamaIndex for most users.\nSOURCE: https://github.com/run-llama/llama_index/blob/main/docs/docs/getting_started/installation.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install llama-index\n```\n\n----------------------------------------\n\nTITLE: Installing LlamaIndex Core Library\nDESCRIPTION: This Python command installs the core `llama-index` library, which is essential for building applications with LlamaIndex. It provides the foundational components for data indexing, querying, and interacting with various LLMs.\nSOURCE: https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/llm/llamafile.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n!pip install llama-index\n```\n\n----------------------------------------\n\nTITLE: Installing LlamaIndex Core Library (Python)\nDESCRIPTION: Installs the core LlamaIndex library using pip. This is a fundamental dependency for any LlamaIndex application.\nSOURCE: https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/vector_stores/TencentVectorDBIndexDemo.ipynb#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n!pip install llama-index\n```\n\n----------------------------------------\n\nTITLE: Installing LlamaIndex Core Library (Python)\nDESCRIPTION: Installs the core `llama-index` library, which is a fundamental dependency for building applications with LlamaIndex. This package provides the base functionalities required for various LLM operations and data indexing.\nSOURCE: https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/llm/yi.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n!pip install llama-index\n```\n\n----------------------------------------\n\nTITLE: Installing LlamaIndex Python Library\nDESCRIPTION: This command installs the LlamaIndex Python library using pip, the standard package installer for Python. It is the first step to set up LlamaIndex in your development environment, enabling access to its high-level and low-level APIs.\nSOURCE: https://github.com/run-llama/llama_index/blob/main/docs/docs/index.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install llama-index\n```\n\n----------------------------------------\n\nTITLE: Building VectorStoreIndex from Documents (Python)\nDESCRIPTION: Constructs a `VectorStoreIndex` from the loaded `documents`. This process involves embedding the document content into vector representations, which are then stored in the index for efficient semantic search and retrieval during query operations.\nSOURCE: https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/vector_stores/SimpleIndexDemoLlama2.ipynb#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nindex = VectorStoreIndex.from_documents(documents)\n```\n\n----------------------------------------\n\nTITLE: Complete RAG Application with ASI and OpenAI Embeddings (Python)\nDESCRIPTION: This comprehensive example illustrates how to build a Retrieval Augmented Generation (RAG) application using ASI as the Large Language Model (LLM) and OpenAI for embeddings within LlamaIndex. It covers loading documents, parsing them into nodes, creating an embedding model, initializing the ASI LLM, building a vector store index, and performing a streaming query. This setup highlights ASI's flexibility as a drop-in replacement for other LLMs in a RAG pipeline.\nSOURCE: https://github.com/run-llama/llama_index/blob/main/llama-index-integrations/llms/llama-index-llms-asi/README.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.llms.asi import ASI\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\nfrom llama_index.core.node_parser import SentenceSplitter\n\n# Load documents\ndocuments = SimpleDirectoryReader(\"./data\").load_data()\n\n# Create parser\nparser = SentenceSplitter(chunk_size=1024)\n\n# Create embeddings model (still using OpenAI for embeddings)\nembedding_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n\n# Create ASI LLM (instead of OpenAI)\nllm = ASI(model=\"asi1-mini\", api_key=\"your_asi_api_key\")\n\n# Create index\nnodes = parser.get_nodes_from_documents(documents)\nindex = VectorStoreIndex(nodes, embed_model=embedding_model)\n\n# Create query engine with ASI as the LLM\nquery_engine = index.as_query_engine(llm=llm)\n\n# Query with streaming\nresponse = query_engine.query(\n    \"What information is in these documents?\", streaming=True\n)\n\n# Process streaming response\nfor token in response.response_gen:\n    print(token, end=\"\", flush=True)\n```\n\n----------------------------------------\n\nTITLE: Defining a Modular RAG Workflow Class in LlamaIndex\nDESCRIPTION: This snippet defines the `RAGWorkflow` class, inheriting from `Workflow`, which orchestrates a multi-step RAG process. It includes methods for document ingestion (`ingest`), information retrieval (`retrieve`), reranking of retrieved nodes (`rerank`), and synthesizing a final response (`synthesize`), utilizing LlamaIndex components like `SimpleDirectoryReader`, `VectorStoreIndex`, `LLMRerank`, and `CompactAndRefine`. Each method is decorated with `@step` and handles specific events and context.\nSOURCE: https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/workflow/rag.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.core import SimpleDirectoryReader, VectorStoreIndex\nfrom llama_index.core.response_synthesizers import CompactAndRefine\nfrom llama_index.core.postprocessor.llm_rerank import LLMRerank\nfrom llama_index.core.workflow import (\n    Context,\n    Workflow,\n    StartEvent,\n    StopEvent,\n    step,\n)\n\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.embeddings.openai import OpenAIEmbedding\n\n\nclass RAGWorkflow(Workflow):\n    @step\n    async def ingest(self, ctx: Context, ev: StartEvent) -> StopEvent | None:\n        \"\"\"Entry point to ingest a document, triggered by a StartEvent with `dirname`.\"\"\"\n        dirname = ev.get(\"dirname\")\n        if not dirname:\n            return None\n\n        documents = SimpleDirectoryReader(dirname).load_data()\n        index = VectorStoreIndex.from_documents(\n            documents=documents,\n            embed_model=OpenAIEmbedding(model_name=\"text-embedding-3-small\"),\n        )\n        return StopEvent(result=index)\n\n    @step\n    async def retrieve(\n        self, ctx: Context, ev: StartEvent\n    ) -> RetrieverEvent | None:\n        \"Entry point for RAG, triggered by a StartEvent with `query`.\"\n        query = ev.get(\"query\")\n        index = ev.get(\"index\")\n\n        if not query:\n            return None\n\n        print(f\"Query the database with: {query}\")\n\n        # store the query in the global context\n        await ctx.set(\"query\", query)\n\n        # get the index from the global context\n        if index is None:\n            print(\"Index is empty, load some documents before querying!\")\n            return None\n\n        retriever = index.as_retriever(similarity_top_k=2)\n        nodes = await retriever.aretrieve(query)\n        print(f\"Retrieved {len(nodes)} nodes.\")\n        return RetrieverEvent(nodes=nodes)\n\n    @step\n    async def rerank(self, ctx: Context, ev: RetrieverEvent) -> RerankEvent:\n        # Rerank the nodes\n        ranker = LLMRerank(\n            choice_batch_size=5, top_n=3, llm=OpenAI(model=\"gpt-4o-mini\")\n        )\n        print(await ctx.get(\"query\", default=None), flush=True)\n        new_nodes = ranker.postprocess_nodes(\n            ev.nodes, query_str=await ctx.get(\"query\", default=None)\n        )\n        print(f\"Reranked nodes to {len(new_nodes)}\")\n        return RerankEvent(nodes=new_nodes)\n\n    @step\n    async def synthesize(self, ctx: Context, ev: RerankEvent) -> StopEvent:\n        \"\"\"Return a streaming response using reranked nodes.\"\"\"\n        llm = OpenAI(model=\"gpt-4o-mini\")\n        summarizer = CompactAndRefine(llm=llm, streaming=True, verbose=True)\n        query = await ctx.get(\"query\", default=None)\n\n        response = await summarizer.asynthesize(query, nodes=ev.nodes)\n        return StopEvent(result=response)\n```\n\n----------------------------------------\n\nTITLE: Installing LlamaIndex Core Package (Python)\nDESCRIPTION: Installs the core `llama-index` package using pip, which provides the fundamental components, including the `GuidelineEvaluator`. This is a primary dependency for running the notebook's examples.\nSOURCE: https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/evaluation/guideline_eval.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n!pip install llama-index\n```\n\n----------------------------------------\n\nTITLE: Installing LlamaIndex Core Package\nDESCRIPTION: This Python command installs the core LlamaIndex library. It is a fundamental dependency for using any LlamaIndex components, including the LlamafileEmbedding class, to build applications with LLMs.\nSOURCE: https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/embeddings/llamafile.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n!pip install llama-index\n```\n\n----------------------------------------\n\nTITLE: Installing LlamaIndex in Python\nDESCRIPTION: This snippet installs the LlamaIndex library using pip, which is a prerequisite for running the examples in this notebook, especially when using Google Colab.\nSOURCE: https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/customization/prompts/chat_prompts.ipynb#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n%pip install llama-index\n```\n\n----------------------------------------\n\nTITLE: Importing LlamaIndex Core Modules - Python\nDESCRIPTION: Imports essential classes and functions from LlamaIndex, including readers, storage contexts, various index types (VectorStoreIndex, SimpleKeywordTableIndex, SummaryIndex), OpenAI LLM, and utility functions for notebook display. These imports are foundational for building LlamaIndex applications.\nSOURCE: https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/docstore/DynamoDBDocstoreDemo.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.core import SimpleDirectoryReader, StorageContext\nfrom llama_index.core import VectorStoreIndex, SimpleKeywordTableIndex\nfrom llama_index.core import SummaryIndex\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.core.response.notebook_utils import display_response\nfrom llama_index.core import Settings\n```\n\n----------------------------------------\n\nTITLE: Initializing LlamaIndex Function Calling Agent with Tools (Python)\nDESCRIPTION: This snippet demonstrates how to initialize a `FuncationCallingAgent` in LlamaIndex. It defines two simple Python functions (`add`, `multiply`) and converts them into `FunctionTool` instances. The agent is then configured with an OpenAI LLM, these tools, a timeout, and verbose logging, followed by an initial run.\nSOURCE: https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/workflow/function_calling_agent.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.core.tools import FunctionTool\nfrom llama_index.llms.openai import OpenAI\n\ndef add(x: int, y: int) -> int:\n    \"\"\"Useful function to add two numbers.\"\"\"\n    return x + y\n\ndef multiply(x: int, y: int) -> int:\n    \"\"\"Useful function to multiply two numbers.\"\"\"\n    return x * y\n\ntools = [\n    FunctionTool.from_defaults(add),\n    FunctionTool.from_defaults(multiply),\n]\n\nagent = FuncationCallingAgent(\n    llm=OpenAI(model=\"gpt-4o-mini\"), tools=tools, timeout=120, verbose=True\n)\n\nret = await agent.run(input=\"Hello!\")\n```\n\n----------------------------------------\n\nTITLE: Installing LlamaIndex Core Library with pip\nDESCRIPTION: This command installs the core `llama-index` library using pip, which is the fundamental dependency for building and querying indexes. It's often used in environments like Google Colab or Jupyter notebooks to ensure the library is available.\nSOURCE: https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/citation/pdf_page_reference.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n!pip install llama-index\n```\n\n----------------------------------------\n\nTITLE: Installing Core LlamaIndex Library\nDESCRIPTION: Installs the main LlamaIndex library, which provides the core functionalities for building and querying knowledge-augmented LLM applications. This is a prerequisite for using any LlamaIndex features.\nSOURCE: https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/customization/llms/AzureOpenAI.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n!pip install llama-index\n```\n\n----------------------------------------\n\nTITLE: Building a Vector Store Index with OpenAI (Python)\nDESCRIPTION: This Python snippet demonstrates how to create a simple vector store index using LlamaIndex with OpenAI. It involves setting the OpenAI API key, loading documents from a specified directory using `SimpleDirectoryReader`, and then building the `VectorStoreIndex` from these documents.\nSOURCE: https://github.com/run-llama/llama_index/blob/main/README.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n\ndocuments = SimpleDirectoryReader(\"YOUR_DATA_DIRECTORY\").load_data()\nindex = VectorStoreIndex.from_documents(documents)\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI LLM and Creating LlamaIndex Vector Store\nDESCRIPTION: This snippet initializes an OpenAI Large Language Model (LLM) with the 'gpt-4o' model. It then creates a `VectorStoreIndex` from the loaded documents, which involves embedding the document content. Finally, it configures a `query_engine` from this index, enabling semantic search and retrieval over the indexed data using the specified LLM.\nSOURCE: https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/cookbooks/crewai_llamaindex.ipynb#_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\nllm = OpenAI(model=\"gpt-4o\")\nindex = VectorStoreIndex.from_documents(docs)\nquery_engine = index.as_query_engine(similarity_top_k=5, llm=llm)\n```\n\n----------------------------------------\n\nTITLE: Installing LlamaIndex Core Library - Python\nDESCRIPTION: Installs the core `llama-index` library, which is essential for building and querying vector indexes.\nSOURCE: https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/vector_stores/WeaviateIndexDemo-Hybrid.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n!pip install llama-index\n```\n\n----------------------------------------\n\nTITLE: Installing LlamaIndex Python Package\nDESCRIPTION: Installs the `llama-index` Python package using pip, which includes the `llamaindex-cli` tool necessary for downloading LlamaPacks.\nSOURCE: https://github.com/run-llama/llama_index/blob/main/llama-index-packs/llama-index-packs-streamlit-chatbot/README.md#_snippet_0\n\nLANGUAGE: Bash\nCODE:\n```\npip install llama-index\n```\n\n----------------------------------------\n\nTITLE: Installing LlamaIndex Core Library\nDESCRIPTION: Installs the core `llama-index` library, which is a fundamental dependency for building and running LlamaIndex applications.\nSOURCE: https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/output_parsing/guidance_sub_question.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n!pip install llama-index\n```\n\n----------------------------------------\n\nTITLE: Installing LlamaIndex Core Library - Python\nDESCRIPTION: This command installs the core `llama-index` library, which is essential for using any LlamaIndex functionalities, including chat engines and data indexing. It's a fundamental dependency for the examples in this notebook.\nSOURCE: https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/chat_engine/chat_engine_repl.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n!pip install llama-index\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key in Python\nDESCRIPTION: This snippet sets the OpenAI API key as an environment variable. This is a prerequisite for using OpenAI models with LlamaIndex for tasks like embedding generation and response synthesis.\nSOURCE: https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/node_postprocessor/RecencyPostprocessorDemo.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key Environment Variable\nDESCRIPTION: Initializes a placeholder for the OpenAI API key and sets it as an environment variable, which is required for authenticating requests to the OpenAI API.\nSOURCE: https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/node_postprocessor/rankGPT.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nOPENAI_API_KEY = \"sk-\"\nos.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key in Python\nDESCRIPTION: Imports necessary modules (getpass, os, openai) and prompts the user to securely enter their OpenAI API key, which is then set as an environment variable and assigned to openai.api_key for authentication with the OpenAI service.\nSOURCE: https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/vector_stores/ElasticsearchIndexDemo.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport getpass\nimport os\n\nimport openai\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n\nopenai.api_key = os.environ[\"OPENAI_API_KEY\"]\n```\n\n----------------------------------------\n\nTITLE: Installing LlamaIndex Core Package\nDESCRIPTION: Installs the core `llama-index` library using pip, a fundamental prerequisite for building applications and interacting with various components within the LlamaIndex ecosystem.\nSOURCE: https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/embeddings/upstage.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n!pip install llama-index\n```\n\n----------------------------------------\n\nTITLE: Downloading, Building RAG, and Evaluating with RagEvaluatorPack in Python\nDESCRIPTION: This comprehensive Python snippet demonstrates how to programmatically download the 'CovidQaDataset', build a basic RAG (Retrieval Augmented Generation) system using `VectorStoreIndex`, and then evaluate its performance using the `RagEvaluatorPack`. It highlights parameters for API call batching and sleep times, crucial for managing OpenAI API usage tiers.\nSOURCE: https://github.com/run-llama/llama_index/blob/main/llama-datasets/covidqa/README.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.core.llama_dataset import download_llama_dataset\nfrom llama_index.core.llama_pack import download_llama_pack\nfrom llama_index.core import VectorStoreIndex\n\n# download and install dependencies for benchmark dataset\nrag_dataset, documents = download_llama_dataset(\"CovidQaDataset\", \"./data\")\n\n# build basic RAG system\nindex = VectorStoreIndex.from_documents(documents=documents)\nquery_engine = index.as_query_engine()\n\n# evaluate using the RagEvaluatorPack\nRagEvaluatorPack = download_llama_pack(\n    \"RagEvaluatorPack\", \"./rag_evaluator_pack\"\n)\nrag_evaluator_pack = RagEvaluatorPack(\n    rag_dataset=rag_dataset, query_engine=query_engine, show_progress=True\n)\n\n############################################################################\n# NOTE: If have a lower tier subscription for OpenAI API like Usage Tier 1 #\n# then you'll need to use different batch_size and sleep_time_in_seconds.  #\n# For Usage Tier 1, settings that seemed to work well were batch_size=5,   #\n# and sleep_time_in_seconds=15 (as of December 2023.)                      #\n############################################################################\n\nbenchmark_df = await rag_evaluator_pack.arun(\n    batch_size=40,  # batches the number of openai api calls to make\n    sleep_time_in_seconds=1  # seconds to sleep before making an api call\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring LLM and Embedding Models (Python)\nDESCRIPTION: This code initializes the OpenAI Large Language Model (LLM) and OpenAI Embedding model. It then sets these models globally within LlamaIndex's `Settings`, ensuring they are used by default for subsequent operations like indexing and querying.\nSOURCE: https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/cookbooks/oreilly_course_cookbooks/Module-2/Components_Of_LlamaIndex.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.core import Settings\n\nllm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.2)\nembed_model = OpenAIEmbedding()\n\nSettings.llm = llm\nSettings.embed_model = embed_model\n```\n\n----------------------------------------\n\nTITLE: Install LlamaIndex Core Library (Python)\nDESCRIPTION: Installs the main `llama-index` library, which provides the core functionalities for data indexing, retrieval, and chat engine creation. This is a fundamental prerequisite for any LlamaIndex project.\nSOURCE: https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/chat_engine/chat_engine_context.ipynb#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n!pip install llama-index\n```\n\n----------------------------------------\n\nTITLE: Constructing and Linking Query Pipeline Modules in LlamaIndex (Python)\nDESCRIPTION: This code defines a LlamaIndex `QueryPipeline` by assembling various pre-defined modules, including input, query rewriting, LLM, multiple retrievers, an argument packing component (`argpack_component`), a reranker, and the custom `response_component`. It then establishes the data flow between these modules using `add_link`, demonstrating how to chain operations like query rewriting, parallel retrieval, node joining, reranking, and final response generation.\nSOURCE: https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/pipeline/query_pipeline_memory.ipynb#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\npipeline = QueryPipeline(\n    modules={\n        \"input\": input_component,\n        \"rewrite_template\": rewrite_template,\n        \"llm\": llm,\n        \"rewrite_retriever\": retriever,\n        \"query_retriever\": retriever,\n        \"join\": argpack_component,\n        \"reranker\": reranker,\n        \"response_component\": response_component,\n    },\n    verbose=False,\n)\n\n# run both retrievers -- once with the hallucinated query, once with the real query\npipeline.add_link(\n    \"input\", \"rewrite_template\", src_key=\"query_str\", dest_key=\"query_str\"\n)\npipeline.add_link(\n    \"input\",\n    \"rewrite_template\",\n    src_key=\"chat_history_str\",\n    dest_key=\"chat_history_str\",\n)\npipeline.add_link(\"rewrite_template\", \"llm\")\npipeline.add_link(\"llm\", \"rewrite_retriever\")\npipeline.add_link(\"input\", \"query_retriever\", src_key=\"query_str\")\n\n# each input to the argpack component needs a dest key -- it can be anything\n# then, the argpack component will pack all the inputs into a single list\npipeline.add_link(\"rewrite_retriever\", \"join\", dest_key=\"rewrite_nodes\")\npipeline.add_link(\"query_retriever\", \"join\", dest_key=\"query_nodes\")\n\n# reranker needs the packed nodes and the query string\npipeline.add_link(\"join\", \"reranker\", dest_key=\"nodes\")\npipeline.add_link(\n    \"input\", \"reranker\", src_key=\"query_str\", dest_key=\"query_str\"\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Tools for MistralAI Function Calling\nDESCRIPTION: This code defines two Python functions, `multiply` and `mystery`, and wraps them as `FunctionTool` objects for use with LlamaIndex. It then initializes the MistralAI LLM with the `mistral-large-latest` model, which natively supports function calling.\nSOURCE: https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/llm/mistralai.ipynb#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nfrom llama_index.llms.mistralai import MistralAI\nfrom llama_index.core.tools import FunctionTool\n\n\ndef multiply(a: int, b: int) -> int:\n    \"\"\"Multiple two integers and returns the result integer\"\"\"\n    return a * b\n\n\ndef mystery(a: int, b: int) -> int:\n    \"\"\"Mystery function on two integers.\"\"\"\n    return a * b + a + b\n\n\nmystery_tool = FunctionTool.from_defaults(fn=mystery)\nmultiply_tool = FunctionTool.from_defaults(fn=multiply)\n\nllm = MistralAI(model=\"mistral-large-latest\")\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for API Keys\nDESCRIPTION: Sets environment variables for GitHub and OpenAI API keys. These keys are crucial for authenticating requests to their respective services, enabling data loading and LLM interactions.\nSOURCE: https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/query_engine/multi_doc_auto_retrieval/multi_doc_auto_retrieval.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ[\"GITHUB_TOKEN\"] = \"ghp_...\"\nos.environ[\"OPENAI_API_KEY\"] = \"sk_...\"\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI and Pinecone API Keys (Python)\nDESCRIPTION: This code sets the PINECONE_API_KEY and OPENAI_API_KEY environment variables, which are required for authenticating with the Pinecone vector database and OpenAI API respectively. Replace '...' with your actual API keys.\nSOURCE: https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/agent/openai_agent_query_cookbook.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ[\"PINECONE_API_KEY\"] = \"...\"\nos.environ[\"OPENAI_API_KEY\"] = \"...\"\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key Environment Variable\nDESCRIPTION: This snippet demonstrates how to set the OpenAI API key as an environment variable using Python's `os` module. This is a prerequisite for using OpenAI's models with LlamaIndex agents.\nSOURCE: https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/tools/AgentQL_browser_agent.ipynb#_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\n# set your openai key, if using openai\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n```\n\n----------------------------------------\n\nTITLE: Importing LlamaIndex Components for RAG\nDESCRIPTION: Imports essential LlamaIndex components required for building a Retrieval Augmented Generation (RAG) system. These include `VectorStoreIndex` for managing vector stores, `SimpleDirectoryReader` for loading documents, `resolve_embed_model` for embedding models, and `SentenceSplitter` for node parsing.\nSOURCE: https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/llm/predibase.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\nfrom llama_index.core.embeddings import resolve_embed_model\nfrom llama_index.core.node_parser import SentenceSplitter\n```\n\n----------------------------------------\n\nTITLE: Installing LlamaIndex Core Library (Python)\nDESCRIPTION: Installs the core `llama-index` library, which is essential for all LlamaIndex functionalities, including index creation, query engines, and data handling. This is a fundamental dependency for the project.\nSOURCE: https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/vector_stores/chroma_auto_retriever.ipynb#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n!pip install llama-index\n```\n\n----------------------------------------\n\nTITLE: Installing Required LlamaIndex and CrewAI Libraries\nDESCRIPTION: This snippet installs all necessary Python packages for the project. It includes `llama-index-core` for core LlamaIndex functionalities, `llama-index-readers-file` for file reading, `llama-index-tools-wolfram-alpha` for Wolfram Alpha integration, and `crewai[tools]` to install CrewAI along with its tool-related dependencies.\nSOURCE: https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/cookbooks/crewai_llamaindex.ipynb#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n!pip install llama-index-core\n!pip install llama-index-readers-file\n!pip install llama-index-tools-wolfram-alpha\n!pip install 'crewai[tools]'\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI LLM (Python)\nDESCRIPTION: This code initializes an OpenAI Large Language Model (LLM) instance using `gpt-3.5-turbo`. This LLM will be used by the query engine for generating responses.\nSOURCE: https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/vector_stores/MilvusAsyncAPIDemo.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.llms.openai import OpenAI\n\nllm = OpenAI(model=\"gpt-3.5-turbo\")\n```\n\n----------------------------------------\n\nTITLE: Configuring and Using OpenAI LLM in LlamaIndex\nDESCRIPTION: This snippet demonstrates how to set OpenAI as the global default LLM in LlamaIndex settings, perform a local text completion using the OpenAI LLM, and integrate an LLM into query and chat engines for specific usage contexts. It showcases both global and local LLM configuration patterns.\nSOURCE: https://github.com/run-llama/llama_index/blob/main/docs/docs/module_guides/models/llms.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nfrom llama_index.core import Settings\nfrom llama_index.llms.openai import OpenAI\n\n# changing the global default\nSettings.llm = OpenAI()\n\n# local usage\nresp = OpenAI().complete(\"Paul Graham is \")\nprint(resp)\n\n# per-query/chat engine\nquery_engine = index.as_query_engine(..., llm=llm)\nchat_engine = index.as_chat_engine(..., llm=llm)\n```\n\n----------------------------------------\n\nTITLE: Querying LlamaIndex with Metadata Filters (Python)\nDESCRIPTION: This snippet demonstrates how to configure a LlamaIndex retriever with advanced metadata filtering capabilities. It sets `similarity_top_k` to 3 and applies a complex set of filters using `MetadataFilters` with an 'and' condition, combining an `ExactMatchFilter` for `file_name` and `MetadataFilter` instances for `updated_at` (greater than or equal to a timestamp) and `text` (text match).\nSOURCE: https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/vector_stores/RedisIndexDemo.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.core.vector_stores import (\n    MetadataFilters,\n    MetadataFilter,\n    ExactMatchFilter,\n)\n\nretriever = index.as_retriever(\n    similarity_top_k=3,\n    filters=MetadataFilters(\n        filters=[\n            ExactMatchFilter(key=\"file_name\", value=\"paul_graham_essay.txt\"),\n            MetadataFilter(\n                key=\"updated_at\",\n                value=date_to_timestamp(\"2023-01-01\"),\n                operator=\">=\",\n            ),\n            MetadataFilter(\n                key=\"text\",\n                value=\"learn\",\n                operator=\"text_match\",\n            )\n        ],\n        condition=\"and\"\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Anthropic API Key Environment Variable\nDESCRIPTION: This code sets the Anthropic API key as an environment variable, which is a common practice for securely providing credentials to applications. The LlamaIndex Anthropic LLM client will automatically pick up this key.\nSOURCE: https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/llm/anthropic.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = \"sk-...\"\n```\n\n----------------------------------------\n\nTITLE: Setting Netmind API Key Environment Variable (Python)\nDESCRIPTION: This Python snippet demonstrates how to set your Netmind API key as an environment variable. This is a common and secure practice for authenticating API requests, preventing the key from being hardcoded directly in the application.\nSOURCE: https://github.com/run-llama/llama_index/blob/main/llama-index-integrations/embeddings/llama-index-embeddings-netmind/README.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ[\"NETMIND_API_KEY\"] = \"you_api_key\"\n```\n\n----------------------------------------\n\nTITLE: Querying Vector Index in LlamaIndex - Python\nDESCRIPTION: This code initializes a query engine from a `vector_index` and performs a query to retrieve information based on semantic similarity. Vector indices are ideal for questions requiring deep contextual understanding and similarity-based retrieval.\nSOURCE: https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/docstore/RedisDocstoreIndexStoreDemo.ipynb#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nquery_engine = vector_index.as_query_engine()\nvector_response = query_engine.query(\"What did the author do growing up?\")\n```\n\n----------------------------------------\n\nTITLE: Loading Documents and Creating Vector Store Index\nDESCRIPTION: Loads documents from a specified file path using `SimpleDirectoryReader` and then creates a `VectorStoreIndex` from these documents. This index is crucial for efficient retrieval of relevant information during query processing.\nSOURCE: https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/customization/llms/AzureOpenAI.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndocuments = SimpleDirectoryReader(\n    input_files=[\"../../data/paul_graham/paul_graham_essay.txt\"]\n).load_data()\nindex = VectorStoreIndex.from_documents(documents)\n```\n\n----------------------------------------\n\nTITLE: Defining DocsAssistantWorkflow for Multi-Branch Operations - Python\nDESCRIPTION: This class defines a `DocsAssistantWorkflow` that acts as an intelligent documentation assistant. It uses an OpenAI LLM to evaluate incoming queries and decide on multiple concurrent actions, such as writing content from LlamaIndex or Weaviate URLs to respective document collections, or answering questions using a query agent. The `send_event` method is crucial for triggering multiple steps from a single evaluation.\nSOURCE: https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/agent/multi_agent_workflow_with_weaviate_queryagent.ipynb#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nclass ActionCompleted(Event):\n    result: str\n\n\nclass DocsAssistantWorkflow(Workflow):\n    def __init__(self, *args, **kwargs):\n        self.llm = OpenAIResponses(model=\"gpt-4.1-mini\")\n        self.system_prompt = \"\"\"You are a docs assistant. You evaluate incoming queries and break them down to subqueries when needed.\n                      You decide on the next best course of action. Overall, here are the options:\n                      - You can write the contents of a URL to llamaindex docs (if it's a llamaindex url)\n                      - You can write the contents of a URL to weaviate docs (if it's a weaviate url)\n                      - You can answer a question about llamaindex and weaviate using the QueryAgent\"\"\"\n        super().__init__(*args, **kwargs)\n\n    @step\n    async def start(self, ctx: Context, ev: StartEvent) -> EvaluateQuery:\n        return EvaluateQuery(query=ev.query)\n\n    @step\n    async def evaluate_query(\n        self, ctx: Context, ev: EvaluateQuery\n    ) -> QueryAgentEvent | WriteLlamaIndexDocsEvent | WriteWeaviateDocsEvent | None:\n        await ctx.set(\"results\", [])\n        sllm = self.llm.as_structured_llm(Actions)\n        response = await sllm.achat(\n            [\n                ChatMessage(role=\"system\", content=self.system_prompt),\n                ChatMessage(role=\"user\", content=ev.query),\n            ]\n        )\n        actions = response.raw.actions\n        await ctx.set(\"num_events\", len(actions))\n        await ctx.set(\"results\", [])\n        print(actions)\n        for action in actions:\n            if isinstance(action, SaveToLlamaIndexDocs):\n                ctx.send_event(\n                    WriteLlamaIndexDocsEvent(urls=action.llama_index_urls)\n                )\n            elif isinstance(action, SaveToWeaviateDocs):\n                ctx.send_event(\n                    WriteWeaviateDocsEvent(urls=action.weaviate_urls)\n                )\n            elif isinstance(action, Ask):\n                for query in action.queries:\n                    ctx.send_event(QueryAgentEvent(query=query))\n\n    @step\n    async def write_li_docs(\n        self, ctx: Context, ev: WriteLlamaIndexDocsEvent\n    ) -> ActionCompleted:\n        print(f\"Writing {ev.urls} to LlamaIndex Docs\")\n        write_webpages_to_weaviate(\n            client, urls=ev.urls, collection_name=\"LlamaIndexDocs\"\n        )\n        results = await ctx.get(\"results\")\n        results.append(f\"Wrote {ev.urls} it LlamaIndex Docs\")\n        return ActionCompleted(result=f\"Writing {ev.urls} to LlamaIndex Docs\")\n\n    @step\n    async def write_weaviate_docs(\n        self, ctx: Context, ev: WriteWeaviateDocsEvent\n    ) -> ActionCompleted:\n        print(f\"Writing {ev.urls} to Weaviate Docs\")\n        write_webpages_to_weaviate(\n            client, urls=ev.urls, collection_name=\"WeaviateDocs\"\n        )\n        results = await ctx.get(\"results\")\n        results.append(f\"Wrote {ev.urls} it Weavite Docs\")\n        return ActionCompleted(result=f\"Writing {ev.urls} to Weaviate Docs\")\n\n    @step\n    async def query_agent(\n        self, ctx: Context, ev: QueryAgentEvent\n    ) -> ActionCompleted:\n        print(f\"Sending {ev.query} to agent\")\n        response = weaviate_agent.run(ev.query)\n        results = await ctx.get(\"results\")\n        results.append(f\"QueryAgent responded with:\\n {response.final_answer}\")\n        return ActionCompleted(result=f\"Sending `'\" + ev.query + \"`' to agent\")\n\n    @step\n    async def collect(\n        self, ctx: Context, ev: ActionCompleted\n    ) -> StopEvent | None:\n        num_events = await ctx.get(\"num_events\")\n        evs = ctx.collect_events(ev, [ActionCompleted] * num_events)\n        if evs is None:\n            return None\n        return StopEvent(result=[ev.result for ev in evs])\n\n\neverything_docs_agent = DocsAssistantWorkflow(timeout=None)\n```\n\n----------------------------------------\n\nTITLE: Applying Nested Metadata Filters in LlamaIndex\nDESCRIPTION: This snippet demonstrates how to apply complex, nested metadata filters in LlamaIndex, mirroring the SQL example. It combines a date range filter (AND condition) and an author filter (OR condition) using an outer 'and' condition, then retrieves nodes based on this intricate logic.\nSOURCE: https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/vector_stores/postgres.ipynb#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\nfilters = MetadataFilters(\n    filters=[\n        MetadataFilters(\n            filters=[\n                MetadataFilter(\n                    key=\"commit_date\", value=\"2023-08-01\", operator=\">=\"\n                ),\n                MetadataFilter(\n                    key=\"commit_date\", value=\"2023-08-15\", operator=\"<=\"\n                )\n            ],\n            condition=\"and\"\n        ),\n        MetadataFilters(\n            filters=[\n                MetadataFilter(key=\"author\", value=\"mats@timescale.com\"),\n                MetadataFilter(key=\"author\", value=\"sven@timescale.com\")\n            ],\n            condition=\"or\"\n        )\n    ],\n    condition=\"and\"\n)\n\nretriever = index.as_retriever(\n    similarity_top_k=10,\n    filters=filters,\n)\n\nretrieved_nodes = retriever.retrieve(\"What is this software project about?\")\n\nfor node in retrieved_nodes:\n    print(node.node.metadata)\n```\n\n----------------------------------------\n\nTITLE: Parsing Documents into Nodes with SentenceSplitter\nDESCRIPTION: This snippet uses `SentenceSplitter` from LlamaIndex to break down the loaded `documents` into smaller, manageable `nodes`. `SentenceSplitter` typically splits text based on sentence boundaries, which is a common preprocessing step for creating embeddings and improving retrieval accuracy in RAG applications.\nSOURCE: https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/docstore/CloudSQLPgDocstoreDemo.ipynb#_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\nfrom llama_index.core.node_parser import SentenceSplitter\n\nnodes = SentenceSplitter().get_nodes_from_documents(documents)\n```\n\n----------------------------------------\n\nTITLE: Initializing VectorStoreIndex with Documents in Python\nDESCRIPTION: Demonstrates the simplest way to create a VectorStoreIndex by loading documents from a directory using `SimpleDirectoryReader` and then building the index directly from these documents. This method automatically handles document splitting and node creation.\nSOURCE: https://github.com/run-llama/llama_index/blob/main/docs/docs/module_guides/indexing/vector_store_index.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n\n# Load documents and build index\ndocuments = SimpleDirectoryReader(\n    \"../../examples/data/paul_graham\"\n).load_data()\nindex = VectorStoreIndex.from_documents(documents)\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key in Python\nDESCRIPTION: This code sets the `OPENAI_API_KEY` environment variable, which is required for authenticating with the OpenAI API. The RAFT dataset creation process, specifically when using GPT-4, relies on this key for making API calls. Users must replace `<YOUR OPENAI API KEY>` with their actual key.\nSOURCE: https://github.com/run-llama/llama_index/blob/main/llama-index-packs/llama-index-packs-raft-dataset/examples/raft_dataset.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = \"<YOUR OPENAI API KEY>\"\n```\n\n----------------------------------------\n\nTITLE: Setting Perplexity API Key from Environment\nDESCRIPTION: This Python snippet demonstrates how to securely set the Perplexity API key as an environment variable. It prompts the user for the key if it's not already present, ensuring sensitive information is not hardcoded.\nSOURCE: https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/llm/perplexity.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport getpass\nimport os\n\nif \"PPLX_API_KEY\" not in os.environ:\n    os.environ[\"PPLX_API_KEY\"] = getpass.getpass(\n        \"Enter your Perplexity API key: \"\n    )\n```\n\n----------------------------------------\n\nTITLE: Building VectorStoreIndex and Query Engines for RAG (Python)\nDESCRIPTION: Creates `VectorStoreIndex` instances from the loaded Uber and Lyft documents, then converts them into `QueryEngine`s with a specified `similarity_top_k`. These query engines enable retrieval-augmented generation on the financial data, allowing for semantic search.\nSOURCE: https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/cookbooks/oreilly_course_cookbooks/Module-6/Agents.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.core import VectorStoreIndex\n\nuber_index = VectorStoreIndex.from_documents(uber_docs)\nuber_query_engine = uber_index.as_query_engine(similarity_top_k=3)\n\nlyft_index = VectorStoreIndex.from_documents(lyft_docs)\nlyft_query_engine = lyft_index.as_query_engine(similarity_top_k=3)\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key Environment Variable\nDESCRIPTION: Sets the `OPENAI_API_KEY` environment variable, which is required for LlamaIndex to authenticate with the OpenAI API for embedding and language model operations. Users must replace `<your openai key>` with their actual OpenAI API key.\nSOURCE: https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/vector_stores/DocArrayInMemoryIndexDemo.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = \"<your openai key>\"\n```\n\n----------------------------------------\n\nTITLE: Linking Modules to Form a Full RAG Pipeline DAG in LlamaIndex\nDESCRIPTION: Establishes explicit links between the previously added modules in the `QueryPipeline` to form a complete RAG DAG. It defines the flow from prompt generation to LLM, retrieval, reranking (with separate inputs for nodes and query string), and finally response synthesis, demonstrating complex data flow.\nSOURCE: https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/pipeline/query_pipeline.ipynb#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\np.add_link(\"prompt_tmpl\", \"llm\")\np.add_link(\"llm\", \"retriever\")\np.add_link(\"retriever\", \"reranker\", dest_key=\"nodes\")\np.add_link(\"llm\", \"reranker\", dest_key=\"query_str\")\np.add_link(\"reranker\", \"summarizer\", dest_key=\"nodes\")\np.add_link(\"llm\", \"summarizer\", dest_key=\"query_str\")\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Query Engine Class - Python\nDESCRIPTION: Defines the `MyQueryEngine` class, a custom query engine that encapsulates retrieval and hierarchical summarization logic. It includes an `__init__` method for configuration and both synchronous (`query`) and asynchronous (`aquery`) methods for executing queries, returning a `Response` object.\nSOURCE: https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/low_level/response_synthesis.ipynb#_snippet_37\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.core.retrievers import BaseRetriever\nfrom llama_index.core.llms import LLM\nfrom dataclasses import dataclass\nfrom typing import Optional, List\n\n\n@dataclass\nclass Response:\n    response: str\n    source_nodes: Optional[List] = None\n\n    def __str__(self):\n        return self.response\n\n\nclass MyQueryEngine:\n    \"\"\"My query engine.\n\n    Uses the tree summarize response synthesis module by default.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        retriever: BaseRetriever,\n        qa_prompt: PromptTemplate,\n        llm: LLM,\n        num_children=10,\n    ) -> None:\n        self._retriever = retriever\n        self._qa_prompt = qa_prompt\n        self._llm = llm\n        self._num_children = num_children\n\n    def query(self, query_str: str):\n        retrieved_nodes = self._retriever.retrieve(query_str)\n        response_txt, _ = generate_response_hs(\n            retrieved_nodes,\n            query_str,\n            self._qa_prompt,\n            self._llm,\n            num_children=self._num_children,\n        )\n        response = Response(response_txt, source_nodes=retrieved_nodes)\n        return response\n\n    async def aquery(self, query_str: str):\n        retrieved_nodes = await self._retriever.aretrieve(query_str)\n        response_txt, _ = await agenerate_response_hs(\n            retrieved_nodes,\n            query_str,\n            self._qa_prompt,\n            self._llm,\n            num_children=self._num_children,\n        )\n        response = Response(response_txt, source_nodes=retrieved_nodes)\n        return response\n```\n\n----------------------------------------\n\nTITLE: Loading Documents and Building Vector Index (Python)\nDESCRIPTION: Loads documents from the specified directory using `SimpleDirectoryReader` and then constructs a `VectorStoreIndex` from these documents, preparing them for similarity search.\nSOURCE: https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/node_postprocessor/ColbertRerank.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndocuments = SimpleDirectoryReader(\"./data/paul_graham/\").load_data()\n\n# build index\nindex = VectorStoreIndex.from_documents(documents=documents)\n```\n\n----------------------------------------\n\nTITLE: Installing LlamaIndex Core with pip\nDESCRIPTION: This command installs the core `llama-index` library using `pip`, a prerequisite for utilizing LlamaIndex functionalities, often used in notebook environments with `!` for shell commands.\nSOURCE: https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/embeddings/netmind.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n!pip install llama-index\n```\n\n----------------------------------------\n\nTITLE: Streaming Text Completion from OpenRouter LLM - Python\nDESCRIPTION: Demonstrates streaming a text completion response. The `llm.stream_complete()` method is used with a prompt, and the response deltas are iterated and printed as they become available, similar to chat streaming.\nSOURCE: https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/llm/openrouter.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nresp = llm.stream_complete(\"Tell me a story in 250 words\")\nfor r in resp:\n    print(r.delta, end=\"\")\n```\n\n----------------------------------------\n\nTITLE: Chaining Prompt and LLM in QueryPipeline (Python)\nDESCRIPTION: This snippet demonstrates a basic `QueryPipeline` by chaining a `PromptTemplate` with an `OpenAI` LLM. It defines a prompt string with a placeholder, initializes the prompt and LLM, and then creates a pipeline where the output of the prompt feeds directly into the LLM. The `verbose=True` flag enables detailed logging of pipeline execution.\nSOURCE: https://github.com/run-llama/llama_index/blob/main/llama-index-integrations/callbacks/llama-index-callbacks-arize-phoenix/examples/query_pipeline.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# try chaining basic prompts\nprompt_str = \"Please generate related movies to {movie_name}\"\nprompt_tmpl = PromptTemplate(prompt_str)\nllm = OpenAI(model=\"gpt-3.5-turbo\")\n\np = QueryPipeline(chain=[prompt_tmpl, llm], verbose=True)\n```\n\n----------------------------------------\n\nTITLE: Configuring LlamaIndex RAG Pipeline Components - Python\nDESCRIPTION: This snippet demonstrates the setup of a LlamaIndex Retrieval Augmented Generation (RAG) pipeline. It involves creating text nodes from documents, constructing a VectorStoreIndex, configuring a VectorIndexRetriever with a specified similarity_top_k, and initializing a response synthesizer. Finally, it combines these components into a RetrieverQueryEngine for performing queries.\nSOURCE: https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/cookbooks/oreilly_course_cookbooks/Module-2/Components_Of_LlamaIndex.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.core.retrievers import VectorIndexRetriever\nfrom llama_index.core.response_synthesizers import get_response_synthesizer\nfrom llama_index.core.query_engine import RetrieverQueryEngine\n\n# create nodes\nsplitter = TokenTextSplitter(chunk_size=1024, chunk_overlap=20)\nnodes = splitter.get_nodes_from_documents(documents)\n\n# Construct an index by loading documents into a VectorStoreIndex.\nindex = VectorStoreIndex(nodes)\n\n# configure retriever\nretriever = VectorIndexRetriever(index=index, similarity_top_k=3)\n\n# configure response synthesizer\nsynthesizer = get_response_synthesizer(response_mode=\"refine\")\n\n# construct query engine\nquery_engine = RetrieverQueryEngine(\n    retriever=retriever,\n    response_synthesizer=synthesizer,\n)\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key Environment Variable\nDESCRIPTION: Demonstrates how to set the OpenAI API key as an environment variable, which is necessary for LlamaIndex to authenticate with OpenAI services. Users should uncomment and replace 'INSERT OPENAI KEY' with their actual key.\nSOURCE: https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/evaluation/RetryQuery.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# import os\n# os.environ['OPENAI_API_KEY'] = \"INSERT OPENAI KEY\"\n```\n\n----------------------------------------\n\nTITLE: Creating Google Calendar Event with OpenAI Agent (Python)\nDESCRIPTION: This snippet showcases the agent's ability to create a new event on Google Calendar based on a natural language command. It specifies the date, time, duration, and attendees, demonstrating the agent's capability to perform write operations through the integrated tool.\nSOURCE: https://github.com/run-llama/llama_index/blob/main/llama-index-integrations/tools/llama-index-tools-google/examples/google_calendar.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nagent.chat(\n    \"Please create an event for june 15th, 2023 at 5pm for 1 hour and invite\"\n    \" adam@example.com to discuss tax laws\"\n)\n```\n\n----------------------------------------\n\nTITLE: Querying the LlamaIndex RAG Engine\nDESCRIPTION: This snippet executes a query against the previously initialized RAG query engine (`rag`). It uses the `query` variable defined earlier to retrieve contextually relevant information and generate a response augmented by the loaded documents.\nSOURCE: https://github.com/run-llama/llama_index/blob/main/docs/docs/presentations/materials/2024-06-19-georgian-genai-bootcamp.ipynb#_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\nresponse = rag.query(query)\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI LLM and Embedding Models with LlamaIndex in Python\nDESCRIPTION: This snippet imports necessary classes from 'llama_index.embeddings.openai' and 'llama_index.llms.openai' to initialize the 'OpenAIEmbedding' model for generating text embeddings and the 'OpenAI' LLM, specifically using the \"gpt-4o-mini\" model, for language generation tasks. These models are crucial for the RAG pipeline.\nSOURCE: https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/node_parsers/topic_parser.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom llama_index.llms.openai import OpenAI\n\nembed_model = OpenAIEmbedding()\nllm = OpenAI(model=\"gpt-4o-mini\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Query Engine with Reranker and Structured LLM in LlamaIndex (Python)\nDESCRIPTION: This snippet configures the `query_engine` for the RAG pipeline. It sets `similarity_top_k` to retrieve 5 initial nodes, applies the previously defined `reranker` as a node post-processor, and integrates the `sllm` (structured LLM) for generating responses. The `response_mode` is set to `tree_summarize` for comprehensive answer generation.\nSOURCE: https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/structured_outputs/structured_outputs.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nquery_engine = index.as_query_engine(\n    similarity_top_k=5,\n    node_postprocessors=[reranker],\n    llm=sllm,\n    response_mode=\"tree_summarize\"  # you can also select other modes like `compact`, `refine`\n)\n```\n\n----------------------------------------\n\nTITLE: Querying Data with LlamaIndex Query Engine - Python\nDESCRIPTION: This code shows how to execute a query against a previously initialized `query_engine`. It takes a natural language question as input and returns a `response` object containing the answer, demonstrating the core functionality of the query engine.\nSOURCE: https://github.com/run-llama/llama_index/blob/main/docs/docs/module_guides/deploying/query_engine/usage_pattern.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nresponse = query_engine.query(\"Who is Paul Graham?\")\n```\n\n----------------------------------------\n\nTITLE: Loading Documents with SimpleDirectoryReader (Python)\nDESCRIPTION: Utilizes `SimpleDirectoryReader` from LlamaIndex to load all documents found within the specified local directory `./data/paul_graham`. This step prepares the raw text data for subsequent indexing operations.\nSOURCE: https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/observability/TokenCountingHandler.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.core import SimpleDirectoryReader\n\ndocuments = SimpleDirectoryReader(\"./data/paul_graham\").load_data()\n```"
    }
  ]
}
